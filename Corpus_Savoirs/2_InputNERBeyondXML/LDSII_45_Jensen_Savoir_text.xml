<text xml:id="text">
<front>

</front>
<div>
<figure>
<graphic></graphic>
          Image expérimentale (à gauche) et image tirée
          des simulations (à droite) des dépôts de nano-agrégats sur une
          surface de graphite cristallin (<hi>Bardotti</hi>
<hi>et al.</hi>,
          1995).
        </figure>
<p>Les deux images ci-dessus illustrent ce qu’est un
        savoir pour les physiciens : la convergence entre une expérience
        (image de gauche) et des simulations numériques issues d’un modèle
        mathématique (image de droite). La ressemblance de ces deux images
        démontre un fait considéré jusque-là comme impossible : des particules
        contenant des milliers d’atomes peuvent bouger très rapidement sur une
        surface. La création d’un savoir est attestée par la publication de ce
        résultat dans <hi>Physical</hi>
<hi></hi>
<hi>Review</hi>
<hi></hi>
<hi>Letters</hi>
<note>
<p>
<hi>Bardotti</hi>
<hi>et
            al.</hi>, 1995.</p>
</note>, la plus prestigieuse revue de physique. Dans le présent
        texte, je raconte comment s’est faite, concrètement, la convergence de
        ces images, que l’on pourrait presque confondre tellement elles sont
        semblables, tellement le monde artificiel créé par l’ordinateur a été
        rendu similaire au monde naturel des expériences, à moins que ce ne
        soit le contraire.</p>
<p>En <date>janvier
        1993</date>, je partis passer une année de post-doctorat à la Mecque
        intellectuelle des physiciens : les <placename>États-Unis</placename>. Il
        s’agissait d’apprendre la modélisation numérique – jusque-là peu
        utilisée en <placename>France</placename> –, dans un
        environnement scientifique stimulant : l’équipe de <persname>Gene
        Stanley</persname> à l’<placename>université de Boston</placename>.
        Dans mes bagages, j’emportais de mon laboratoire lyonnais une énigme
        que j’espérais résoudre grâce à cette collaboration. Il s’agissait
        d’une contradiction flagrante entre certaines expériences et les
        modèles que nous développions pour comprendre les dépôts de
        nano-agrégats sur des surfaces, thème central de nos recherches. Par
        « nano-agrégat », on entend une petite particule sphérique composée de
        quelques centaines ou milliers d’atomes, le préfixe <hi>nano</hi>
<hi></hi> indiquant que ces agrégats
        ont une taille de l’ordre du nanomètre, soit un milliardième de
        mètre.</p>
<div>
          De l’importance des
          nano-agrégats
          <p>Pour comprendre l’importance de l’énigme, il faut
          replacer nos études dans leur contexte scientifique et
          technologique. Le dépôt de nano-agrégats sur une surface se situe à
          la confluence de deux questionnements majeurs en physique de la
          matière.</p>
<p>La première problématique est fondamentale. Elle
          concerne la transition entre les propriétés d’un atome isolé et
          celles du matériau massif correspondant. Les agrégats représentent
          en effet un état intermédiaire entre ces deux limites, et leurs
          propriétés s’avèrent surprenantes, ne se réduisant pas à une simple
          interpolation entre les propriétés de ces extrêmes. C’est un peu
          comme les caractéristiques des petits groupes d’individus qui ne se
          ramènent pas simplement à celles d’un individu ou à celles de la
          société tout entière. Ainsi, la couleur des nano-agrégats de
          certains matériaux dépend fortement de leur taille : imaginez qu’en
          cassant une bille rouge on en obtienne deux bleues… Autre surprise,
          la température de fusion de ces particules dépend de leur taille
          précise, variant de plusieurs dizaines de degrés lorsque l’on ajoute
          quelques atomes. Depuis les <date>années 1960</date>, diverses techniques furent
          développées pour fabriquer de tels nano-agrégats et les analyser, de
          préférence sans les déposer sur une surface, car cela perturbe leurs
          propriétés.</p>
<p>L’autre grand questionnement, le dépôt de
          particules sur une surface, est important du point de vue
          technologique. En effet, la compréhension des processus qui
          permettent de réaliser des couches très minces (quelques milliers
          d’atomes d’épaisseur) mais parfaitement contrôlées est à la base de
          toute l’industrie électronique. Le fonctionnement de ces objets
          quelque peu magiques repose en effet depuis la <date>Seconde Guerre mondiale</date> sur le même dispositif :
          le transistor, sorte d’interrupteur qui permet d’effectuer des
          calculs élémentaires. La révolution électronique est avant tout le
          résultat de la miniaturisation progressive de ce composant, dont la
          taille a été divisée par cent mille et atteint aujourd’hui la
          centaine de nanomètres grâce à la maîtrise de la matière à ces
          échelles infimes<note>
<p>Pour plus de détails, le lecteur pourra consulter <hi>Jensen</hi>, 1996.</p>
</note>. Désormais, les chercheurs savent que cet accroissement de
          la puissance des ordinateurs sera bientôt confronté à une limite
          physique. En effet, d’ici dix ans environ les transistors auront
          atteint une taille de quelques dizaines de nanomètres ; de nouveaux
          phénomènes physiques apparaîtront alors, empêchant leur bon
          fonctionnement. Ainsi, les courants de fuite à travers des barrières
          isolantes trop fines empêcheront leur fonctionnement comme
          interrupteur. D’où l’importance de trouver de nouveaux dispositifs
          qui remplaceront les transistors, ce qui passe par une compréhension
          des propriétés de la matière à l’échelle du nanomètre.</p>
<p>Nos travaux étaient donc importants du point de
          vue fondamental (la compréhension des propriétés étranges de la
          matière à l’échelle nanométrique) et potentiellement riches
          d’applications technologiques (pour l’industrie électronique). Cela
          permit au Département de physique des matériaux<note>
<p>Rebaptisé depuis la mode des nanosciences en « Laboratoire de
              physique de la matière condensée et des nanostructures ».</p>
</note> de bénéficier de moyens financiers et humains pour avancer
          ses recherches, y compris l’attribution d’un poste de chercheur du
          CNRS que j’eus la chance d’occuper dès <date>septembre 1990</date>.</p>
</div>
<div>
          Les agrégats et la
          neige
          <p>L’objectif général de mon équipe consistait à
          comprendre comment se forme une couche de nano-agrégats. Une
          première idée simple est de comparer sa croissance à la formation
          d’une couche de neige sur un trottoir un jour d’hiver. En gros, les
          agrégats arrivent sur la surface et s’entassent les uns sur les
          autres comme le font les flocons de neige. Mais on peut se demander
          si ce modèle est correct. Sachant que les observations directes de
          la formation de la couche d’agrégats sont impossibles, il faut
          trouver des moyens indirects pour observer la croissance. Un premier
          moyen, adapté au dépôt d’agrégats métalliques, consiste à les
          déposer sur une surface de verre (qui ne conduit pas le courant)
          tout en mesurant le courant qui passe entre deux électrodes
          préalablement disposées aux deux bouts de la plaque de verre.
          Pendant un bon moment, aucun courant ne peut traverser la surface
          isolante du verre, mais après un certain temps on observe un saut de
          courant, attestant la création d’un premier chemin métallique entre
          les électrodes, qui permet au courant de circuler. Revenons à notre
          image de la neige. Imaginez qu’une fourmi veuille traverser la rue
          sans toucher le bitume, en ne marchant que sur la neige : quelle
          fraction de la rue faut-il recouvrir de neige pour qu’elle puisse
          passer d’un trottoir à l’autre ? Bien sûr, si la rue est totalement
          recouverte, c’est possible, mais un modèle mathématique simple, dit
          de « percolation<note>
<p>
<hi>De</hi>
<hi></hi>
<hi>Gennes</hi>, 1976.</p>
</note> », montre qu’il existe un chemin continu reliant les deux
          trottoirs dès que 59 % de la rue a été recouverte.</p>
<p>En partant de ce modèle simple, on peut calculer
          la quantité d’agrégats qu’il faut accumuler sur la surface pour
          observer le saut de courant (le « seuil de conduction »). À notre
          agréable surprise, ce calcul prédit assez bien le seuil de
          conduction et permet également de comprendre la raison pour laquelle
          cette quantité dépend de la taille des agrégats. Comme il est plutôt
          rare qu’un modèle aussi simple puisse expliquer des phénomènes
          réels, nous nous sommes empressés de publier notre modèle<note>
<p>
<hi>Jensen</hi>
<hi>et
              al.</hi>, 1992.</p>
</note>. Hélas, d’autres expériences n’ont pas tardé à nous
          montrer ses limites en établissant que le seuil de conduction dépend
          fortement du flux incident d’agrégats, contrairement à la constance
          prédite par le modèle simple. En effet, le nombre de flocons de
          neige nécessaires pour recouvrir 59 % d’une surface ne dépend pas de
          l’intensité de l’averse de neige ; celle-ci change simplement le
          temps nécessaire pour parvenir à cette couverture.</p>
<p>Ce fait expérimental démontrait que quelque chose
          clochait dans la formule, qu’un phénomène autre qu’un simple
          empilement des agrégats intervenait. Lequel ? Incapables d’observer
          les agrégats à l’œuvre, nous en étions réduits à des hypothèses :
          les agrégats pouvaient bouger sur la surface une fois déposés (ce
          qui n’arrive pas aux flocons !), ils pouvaient s’oxyder
          progressivement pendant le dépôt (la couche d’oxyde empêchant le
          passage du courant), ou bien ils pouvaient coalescer sur la surface,
          comme deux gouttes d’eau qui fusionnent pour n’en donner qu’une,
          plus grosse. Tous ces facteurs étaient susceptibles en principe
          d’affecter le seuil de conduction lorsque le flux incident variait.
          Mais lequel ou lesquels étaient réellement à l’œuvre dans les
          expériences ? Nous n’en savions rien.</p>
</div>
<div>
          Une chambre de dépôt
          virtuelle
          <p>Voilà le mystère que je voulais élucider lors de
          mon année de post-doctorat à <placename>Boston</placename>. Après
          quelques discussions avec mes nouveaux collègues, je décide
          d’explorer l’hypothèse de la diffusion des agrégats. Les raisons de
          ce choix sont assez subjectives, mêlant des influences scientifiques
          et culturelles. D’abord, la diffusion est relativement facile à
          programmer. Elle fait donc partie de la culture du milieu, car des
          modèles antérieurs l’ont prise en compte dans d’autres contextes.
          Ensuite, il existe depuis le début du <date>
<hi>xx</hi>
<hi>e</hi> siècle</date> une théorie solide et simple de la
          diffusion des particules sur une surface. En revanche, les autres
          hypothèses expérimentales (oxydation, coalescence), bien que tout
          aussi vraisemblables du point de vue scientifique, présentent des
          difficultés supplémentaires concernant la théorie ou la
          programmation. Un scientifique voulant publier doit avancer par des
          chemins de moindre résistance… Pourtant, inclure la diffusion des
          agrégats dans la modélisation pouvait apparaître peu raisonnable. En
          effet, la communauté scientifique sait que les atomes individuels
          peuvent diffuser rapidement sur une surface, car ils sont très
          petits et donc sensibles au tangage des surfaces, induit par
          l’agitation thermique des atomes de surface. Cependant, tous les
          experts considéraient impossible la diffusion des nano-agrégats. En
          effet, ceux-ci étant plus gros que les atomes, ils peuvent s’ancrer
          fortement à la surface et rester immobiles.</p>
<p>Malgré la relative simplicité de la diffusion, il
          est impossible de trouver des formules mathématiques donnant le
          seuil de conduction lorsque l’on inclut la diffusion des agrégats.
          D’où l’intérêt des simulations par ordinateur, que le laboratoire de
          <placename>Boston</placename> maîtrise
          parfaitement. Il s’agit de reproduire, <foreign>
<hi>in</hi>
<hi>silico</hi>
</foreign>, ce qui se passe lors du dépôt d’agrégats, en mimant
          les processus un à un et en suivant leur déroulement.</p>
</div>
<div>
          Échec et divine
          surprise
          <p>Je me lance alors tête baissée dans la
          programmation du modèle. Au bout de quatre mois assez pénibles de
          programmation, de bugs et d’erreurs diverses, je parvins enfin à
          écrire un code capable de simuler le dépôt d’agrégats ainsi que leur
          diffusion sur la surface. J’avais en quelque sorte créé une chambre
          de dépôt virtuelle, où je pouvais effectuer des « expériences »
          numériques tout en observant en détail comment la couche d’agrégats
          se forme. Je pouvais changer à loisir le flux d’agrégats incidents
          et observer son influence sur la quantité d’agrégats nécessaire pour
          former un chemin continu, comme dans les expériences réelles. Le
          résultat essentiel est simple : la diffusion des agrégats augmente
          effectivement le seuil de conduction, comme nous en avions
          l’intuition, mais de manière très faible ! La conclusion de ces mois
          de travail dans l’hiver rigoureux de <placename>Boston</placename> est sans
          appel : la diffusion des agrégats ne peut expliquer les résultats
          expérimentaux.</p>
<p>Malgré le découragement – écarter une hypothèse
          ne conduit pas à une publication prestigieuse –, j’informe les
          chercheurs restés à <placename>Lyon</placename> des résultats
          par courrier électronique, qui représentait alors un nouvel outil de
          communication léger et flexible. Cela me permet notamment de leur
          transmettre des images générées par le modèle, que je trouve malgré
          tout assez jolies. <persname>Alain Hoareau</persname>, directeur
          de l’équipe lyonnaise, note une ressemblance frappante entre les
          images du modèle et celles obtenues quelques semaines auparavant par
          <persname>Laurent Bardotti</persname>, alors
          thésard au laboratoire. Il s’agissait d’images résultant d’une
          expérience menée de manière totalement indépendante, et qui visait à
          observer les couches d’agrégats au microscope à effet tunnel. Ce
          type de microscope, à la mode depuis que le prix Nobel a été décerné
          à ses inventeurs en <date>1986</date>,
          permet de mesurer la hauteur de la couche d’agrégats. Pour mener à
          bien ces observations, il faut déposer les agrégats sur une surface
          lisse et sans défauts, le graphite cristallin. Heureux hasard,
          l’interaction entre l’agrégat et cette surface est très faible, ce
          qui permet à ces particules de diffuser fortement, et explique la
          ressemblance avec les images issues du modèle.</p>
<p>Nous reconnaissons là un point d’accrochage
          possible entre des expériences et un modèle, et nous y concentrons
          nos efforts. D’autant plus que, parallèlement, une équipe suisse est
          parvenue à obtenir des images identiques de dépôts atomiques,
          technique nettement plus répandue que celle utilisée à <placename>Lyon</placename>. Notre modèle
          pourrait donc constituer un « point de passage obligé<note>
<p>Ce terme renvoie aux analyses de Bruno Latour et Michel
              Callon, voir par exemple <hi>Latour</hi>,
              2001.</p>
</note> », non seulement pour la petite communauté du dépôt
          d’agrégats, mais également pour celle, autrement plus importante, du
          dépôt d’atomes.</p>
<p>Nous nous dépêchons alors d’oublier la question
          initiale, qui ne présente plus d’intérêt en termes d’impact et donc
          de publication. Cette stratégie, conjuguée à l’influence du groupe
          de <persname>Gene Stanley</persname>, conduit à
          la publication dans la prestigieuse revue <hi>Nature
          </hi>d’une présentation sommaire du modèle<note>
<p>
<hi>Jensen</hi>
<hi>et
              al.</hi>, 1994.</p>
</note>, explicitement relié aux expériences sur les atomes. La
          présentation détaillée du modèle sera publiée quelque temps plus
          tard, dans une revue de physique<note>
<p>
<hi>Jensen</hi>
<hi>et
              al.</hi>, 1994a.</p>
</note>.</p>
<p>Notre travail de modélisation a finalement abouti
          à plusieurs résultats. D’abord, un modèle valide pour la croissance
          des couches d’atomes ou d’agrégats. Ce modèle a été utilisé par de
          nombreux groupes dans le monde pour l’analyse de leurs expériences.
          Nous l’avons bien sûr beaucoup utilisé à <placename>Lyon</placename>, pour mieux
          comprendre les dépôts d’agrégats, aboutissant à un résultat
          étonnant. Contrairement à ce que l’on croyait jusque-là, les
          agrégats sont capables de diffuser très rapidement sur certaines
          surfaces. Ce savoir a été reconnu par la communauté scientifique,
          comme l’atteste le grand nombre de citations (positives) que
          l’article a reçues<note>
<p>Dix citations par an depuis sa publication, chiffre que l’on
              peut comparer aux citations reçues en moyenne par un article de
              physique (moins d’une citation par an) ou par l’article d’Albert
              Fert qui lui valut le prix Nobel en 2007 (deux cents citations
              par an).</p>
</note>. Plusieurs équipes de par le monde ont en effet étendu
          notre étude, en explorant le dépôt d’agrégats d’autres matériaux, en
          imaginant des modèles atomiques capables d’expliquer la diffusion
          rapide des nano-agrégats ou leur coalescence sur la surface.</p>
</div>
<div>
          Comprendre, pour un
          physicien…
          <p>Pour mieux comprendre la démarche qui nous a
          conduits à la création d’un savoir, il convient de replacer notre
          travail dans l’histoire globale de la physique. Voici quatre
          siècles, les physiciens définissaient le savoir légitime comme celui
          qui peut se rattacher à une modélisation rigoureuse. Comme le résume
          <persname>Maurice Clavelin</persname> dans
          son étude sur <persname>Galilée</persname>
<note>
<p>
<hi>Clavelin</hi>, 1996.</p>
</note> : expliquer, pour un physicien, c’est transformer un fait
          physique en un problème mathématique, puis le résoudre grâce aux
          outils rigoureux de cette discipline. Ce n’est pas le lieu ici de
          discuter de la fécondité et des limites globales de cette démarche
          intellectuelle<note>
<p>Je renvoie le lecteur à <hi>Jensen</hi>,
              2001.</p>
</note>. Notre exemple incite plutôt à centrer la discussion
          autour de deux thèmes : le rôle des expériences contrôlées en
          laboratoire et celui des simulations numériques.</p>
<p>Commençons par les expérimentations contrôlées.
          Pour créer un savoir légitime, la physique se limite à la portion de
          réalité qui peut être connectée à un modèle mathématique rigoureux.
          Cela passe notamment par la fabrication d’un monde artificiel
          purifié. Dans notre exemple, il s’agit d’abord de la surface
          cristalline de graphite, qui doit être très plane pour ne pas
          perturber la diffusion <hi>uniforme</hi>
<hi></hi> des agrégats, la seule que l’on peut décrire
          simplement. Les dépôts sur du verre ou sur du carbone désordonné
          (amorphe) sont d’ailleurs toujours mal compris. De plus, le dépôt
          doit être effectué dans un environnement purifié, sous un vide
          poussé qui élimine la contamination des agrégats par l’air ambiant.
          Pour que la jonction entre un modèle et des expériences puisse
          s’établir, il faut bien sûr construire un modèle adapté. Mais le
          plus difficile consiste à réaliser des « manips » suffisamment
          contrôlées pour que la réalité ressemble suffisamment au modèle.</p>
<p>Parmi les expériences menées dans ce cadre
          purifié, on oublie celles qui conduisent à des questions
          intéressantes, mais ne peuvent être reliées à un modèle. Nous avons
          ainsi laissé de côté la question de départ, l’augmentation du seuil
          de conduction avec le flux incident d’agrégats, énigme qui n’est
          toujours pas résolue. Dans les publications, l’effet du flux est
          attribué paresseusement à l’oxydation des nano-agrégats d’antimoine,
          sans trop de preuves. Comme l’a noté l’anthropologue des sciences
          <persname>Pascal Lécaille</persname> lors
          d’un stage dans notre laboratoire<note>
<p>
<hi>Lécaille</hi>, 1996.</p>
</note>, <persname>Laurent Bardotti</persname>
          s’aidait de deux classeurs – étiquetés « utile » et « poubelle » –
          pour ranger les résultats de ses expériences. Le premier permet de
          préclasser ce qui a une chance d’être comparé et amarré aux
          simulations, le deuxième ce qui selon toute vraisemblance restera en
          attente et finira dans une poubelle à la fin de la thèse.</p>
<p>Enfin, pour réussir cette jonction, il existe des
          « tours de main » précis, acquis localement par l’expérience, le
          contact quotidien avec les instruments ou les ordinateurs<note>
<p>
<hi>Collins</hi>, 2001.</p>
</note>. Il a ainsi fallu un temps de tâtonnements à <persname>Laurent Bardotti</persname> avant
          qu’il réussisse à obtenir et à reproduire des surfaces de graphite
          suffisamment lisses. Concrètement, il y est parvenu en les chauffant
          à 500 degrés pendant quelques heures – pour éliminer les impuretés
          chimiques – et en les pelant grâce à du scotch collé à leur surface
          – pour obtenir une surface suffisamment lisse.</p>
<p>Abordons à présent le deuxième moyen dont
          disposent les physiciens pour relier le monde réel et les
          mathématiques. Les simulations représentent aujourd’hui un moyen
          commode d’extension du monde rigoureux des mathématiques cher à
          <persname>Galilée</persname>. Elles
          présentent des similitudes avec les mathématiques, qui les rendent
          suffisamment rigoureuses, et en même temps des différences, qui les
          rendent plus intéressantes dans de nombreux contextes<note>
<p>Sur la nouveauté apportée par les simulations, notamment dans
              le domaine de la fiabilité et de la preuve mathématique, la
              lecture indispensable est <hi>MacKenzie</hi>,
              2001.</p>
</note>. Elles aident donc à établir un pont entre réalité et
          mathématiques de plusieurs manières.</p>
<p>D’abord, bien sûr, en construisant, à l’égal des
          mathématiques, un monde bien défini, rigoureux, sans (trop de)
          surprises une fois qu’il a été défini et exploré. Ainsi, le modèle
          utilisé ici a permis d’interpréter les expériences en les reliant à
          des bases physiques solides (la diffusion des agrégats notamment) de
          manière aussi sûre qu’une théorie mathématique. Pour preuve, notre
          interprétation a tenu face aux objections de nos collègues qui
          doutaient de notre résultat improbable : les agrégats se déplacent
          des millions de fois plus vite que ce qui était accepté
          jusque-là.</p>
<p>Ensuite, autre similarité avec les mathématiques,
          les simulations nous obligent (et nous aident) à transformer nos
          intuitions en des règles rigoureuses, mécaniques. On peut évoquer
          trois manières différentes pour parvenir à ce but : en forçant le
          programme à être cohérent (<hi>via </hi>la
          compilation) ; en explorant tous les cas possibles (qui finissent
          par arriver lors des simulations<note>
<p>Comme la rencontre entre trois particules qui diffusent,
              événement rare que je n’avais pas prévu explicitement et qui a
              « planté » la première version du programme.</p>
</note>) ; en obligeant à préciser les valeurs de tous les
          paramètres qu’on a introduits mais sur lesquels on n’a pas forcément
          envie d’être précis. Écrire un programme opérationnel, c’est
          dialoguer avec un interlocuteur implacable, qui oblige à préciser sa
          pensée, force à décider ce que le modèle indique dans chaque
          configuration, sans ambiguïté<note>
<p>Pour une discussion plus générale du rôle similaire de
              l’écriture par rapport à l’oral, on se référera au livre
              classique de <hi>Goody</hi>, 1979.</p>
</note>.</p>
<p>Les simulations présentent néanmoins des
          différences importantes avec l’approche mathématique habituelle, qui
          vise à établir des formules analytiques générales. Les simulations
          nous permettent en effet de dialoguer avec le monde mathématique de
          manière plus flexible, en utilisant d’autres variables, d’autres
          degrés de liberté plus proches de l’intuition, plus proches aussi de
          ceux utilisés dans les expériences<note>
<p>Pour une discussion approfondie de l’importance croissante
              des simulations dans les sciences, le lecteur pourra consulter
              <hi>Galison</hi>, 1997, <hi>Creager</hi> et <hi>Lunbeck</hi>, 2007. Sur le cas spécifique de
              la physique de la matière, voir <hi>Jensen</hi> et <hi>Blase</hi>, 2002.</p>
</note>. Ainsi pour modéliser la rencontre entre les agrégats qui
          diffusent sur une surface et des îlots déjà formés, les
          mathématiques doivent passer par des coefficients (les « sections
          efficaces de capture ») qui tentent de rendre compte, en moyenne, de
          leur probabilité de rencontre. Ces coefficients n’ont pu être
          calculés, de manière approchée, que pour des îlots supposés
          circulaires, alors que les formes réelles sont plus complexes. Et
          tandis que les simulations obtiennent sans peine ces formes
          fractales (comme celles des illustrations plus haut), les formules
          mathématiques ne peuvent rendre compte que d’îlots circulaires. Or
          il est clair que les images obtenues grâce aux simulations
          permettent une confrontation plus aisée avec les expériences. Dans
          notre exemple, elles ont même donné l’impulsion initiale à la
          connexion entre expériences et modèle, qui ne se serait sans doute
          pas faite autrement…</p>
<p>Un modélisateur est confronté en permanence à
          deux partenaires inflexibles : l’expérimentateur qui lui rappelle
          que son modèle ne doit pas trop s’éloigner du monde expérimental,
          dont il se fait le porte-parole, et l’ordinateur qui l’oblige à
          rester rigoureux, précis. Cette co-construction du modèle est
          essentielle pour que soit réussi l’alignement entre mathématiques et
          expériences. D’où l’importance de contacts quotidiens entre
          expérimentateurs et théoriciens, induisant une meilleure adaptation
          réciproque. Dans ce contexte, l’article scientifique joue un rôle
          important de cristallisateur des tâtonnements quotidiens, en forçant
          l’accord sur certains résultats, obtenus au jour le jour, dans un
          foisonnement d’avis et de contacts.</p>
</div>
<div>
          Épilogue : Simuler la
          société ?
          <p>Ce travail m’avait enseigné la puissance des
          simulations numériques, capables de dépasser les mathématiques dans
          la compréhension de la croissance des couches de nano-agrégats.
          Après une dizaine d’années consacrées à la modélisation des
          nano-agrégats, j’ai eu envie de changer de domaine et de tester
          cette approche sur un champ beaucoup plus difficile : les sciences
          sociales.</p>
<p>Envisagée comme une méthode de compréhension
          générale du social, cette idée peut sembler au mieux naïve, au pis
          irresponsable. En effet, l’étude évoquée précédemment nous a montré
          l’importance de l’expérimentation contrôlée, des situations
          purifiées pour parvenir à la compréhension rigoureuse au sens du
          physicien. Quelle pertinence espérer pour cette approche dans le
          domaine social, heureusement peu contrôlable ? Côté simulations,
          s’il est tentant de fabriquer des « sociétés virtuelles » <foreign>
<hi>in silico</hi>
</foreign>, en partant du comportement des individus, il est clair
          que cet « individualisme méthodologique » est par trop
          simplificateur. Il ignore par exemple un fait essentiel sur le plan
          sociologique : les relations « sociales » sont durables, car elles
          sont portées également par des objets, des institutions,
          c’est-à-dire des structures au niveau mésoscopique qui ne sont pas
          prises en compte dans cette approche<note>
<p>Comme le démontre avec humour Bruno Latour (<hi>Latour</hi>, 2007), l’individualisme
              méthodologique est une approche parfaitement adaptée à une
              société de babouins, dont les structures sociales sont très
              limitées et coûteuses (en temps) à entretenir !</p>
</note>. À quel type de savoir peut-on envisager de parvenir en
          simulant « rigoureusement » le social<note>
<p>Pour une analyse subtile de la « vanité de la rigueur en
              économie », voir <hi>Cartwright</hi>,
              2005.</p>
</note> ?</p>
<p>Je n’ai bien entendu pas de réponse définitive à
          cette question. Cependant, il existe des éléments objectifs qui
          poussent à s’intéresser à des systèmes sociaux spécifiques armés des
          outils de modélisation du physicien. D’abord, depuis quelques
          années, des données numériques de plus en plus nombreuses deviennent
          disponibles. Citons les itinéraires de voitures suivies par GPS, les
          réseaux scientifiques décrits par les articles et leurs citations,
          les transactions sur Internet ou encore la géolocalisation des
          appels téléphoniques par les portables. Face à cette avalanche de
          données, des outils d’analyse quantitatifs soutenus par la puissance
          des ordinateurs peuvent aider à mieux comprendre les mécanismes
          sociaux sous-jacents. Ainsi, la jeune science des réseaux
          complexes<note>
<p>Voir, par exemple, une présentation flatteuse dans <hi>Barabási</hi> et <hi>Bonabeau</hi>, 2003. Pour une analyse plus
              critique, on consultera <hi>Keller</hi>,
              2005.</p>
</note> permet de fouiller des millions d’articles et d’éclairer
          l’histoire des disciplines scientifiques<note>
<p>Pour l’exemple de l’émergence d’une discipline à l’interface
              entre biologie et cancer, voir Cambrosio <hi>et al</hi>., 2006.</p>
</note>, et cela de manière complémentaire à l’approche
          traditionnelle par l’analyse approfondie de quelques textes. Ce
          n’est d’ailleurs pas la première fois que l’avalanche de données
          sociales suscite des tentations du côté des chercheurs en sciences
          exactes. Ainsi, au début du <date>
<hi>xix</hi>
<hi>e</hi> siècle</date>, l’affirmation des bureaucraties
          étatiques avait conduit à la collecte de nombreuses données à
          l’échelle des nations (taux de naissance, de mortalité, de
          suicide…). Ces données ont révélé des régularités inattendues,
          faisant fantasmer des astronomes comme <persname>Quételet</persname>
          qui conçut l’idée d’une « physique sociale ». Le bilan de ce mariage
          entre sciences sociales et mathématiques est fort mince du côté des
          sciences sociales. Il est bien plus intéressant pour les
          mathématiques appliquées et la physique, qui ont hérité d’un nouvel
          outil d’analyse de la variabilité du réel, la loi Normale<note>
<p>Pour une histoire de ces échanges interdisciplinaires, le
              lecteur pourra consulter <hi>Desrosières</hi>,
              2002, ainsi que <hi>Porter</hi>, 1994.</p>
</note>.</p>
<p>L’ordinateur permet également de concevoir des
          sociétés virtuelles <foreign>
<hi>in silico</hi>
</foreign>. Le but consiste à expliquer le niveau macroscopique
          (la société) par le niveau microscopique (les individus et leurs
          interactions). En économie, avec ce type de simulations on peut
          dépasser un grand nombre d’hypothèses restrictives adoptées pour
          simplifier les calculs, menant à cet individu caricatural qu’est
          l’<foreign>
<hi>homo</hi>
<hi>œconomicus</hi>
</foreign>. Les simulations de type « multi-agents » permettent en
          effet d’analyser des systèmes comprenant des individus hétérogènes,
          à rationalité limitée et n’ayant qu’une connaissance imparfaite du
          monde. Pourtant, si les physiciens acceptent couramment que les
          simulations prolongent la rigueur mathématique, cela n’est pas
          (encore) le cas des économistes, qui continuent à privilégier les
          démonstrations mathématiques. Le physicien qui s’y risque doit
          passer par un long travail d’acculturation pour rentrer dans le
          paradigme économique. Cela se traduit notamment par l’utilisation
          des caractéristiques de base des acteurs économiques (utilité
          individuelle, prix de réserve, stratégie d’optimisation…) qui
          rendent le savoir produit par les simulations assimilable par les
          économistes et cumulable avec leurs modèles. Cela est souvent
          frustrant pour le physicien, qui trouve que ces caractéristiques
          sont plus dictées par convenance mathématique que par des données
          empiriques. Mais, à moins de vouloir refonder l’économie sur de
          nouveaux ingrédients de base, les simulations doivent se plier à
          ceux utilisés aujourd’hui, faute de quoi les résultats ne seront pas
          considérés comme un savoir… en économie.</p>
</div>
</div>
<back>
<div>
        Bibliographie
        
</div>
</back>
</text>