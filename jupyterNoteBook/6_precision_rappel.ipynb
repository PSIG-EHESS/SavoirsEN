{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204148bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from nervaluate import Evaluator\n",
    "import argparse\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "'''\n",
    "Ce script utilise nervaluate pour calculer la précision et le rappel\n",
    "du marquage automatique des entités nommées par rapport à un étalon-or. \n",
    "Il recherche un dossier appelé \"Predictions\", qui doit contenir tous les fichiers à évaluer sous la forme suivante:\n",
    "TOKEN   NE-COARSE-LIT   GOLD   VALIDITY\n",
    "Deux       O             O        1\n",
    "mois       O             O        1\n",
    "...\n",
    "Les scores sont générés pour chaque fichier et enregistrés dans un dossier appelé \"Scores\".\n",
    "'''\n",
    "\n",
    "# Initialize the parser\n",
    "parser = argparse.ArgumentParser(description=\"utilise nervaluate pour calculer la précision et le rappel\" )\n",
    "\n",
    "def pretty_print(result, outfile=None):\n",
    "    \"\"\"Affichage plus beau que par défaut\"\"\"\n",
    "    x_name = ['Measure'] + [k for k in result]\n",
    "    y_name = []\n",
    "    rows = []\n",
    "    for evaluation in result:\n",
    "        print(evaluation)\n",
    "        metrics = result[evaluation]\n",
    "        row = []\n",
    "        for metric, score in metrics.items():\n",
    "            if metric not in y_name:\n",
    "                y_name.append(metric)\n",
    "            row.append(round(score, 3)) # Arrondir\n",
    "        rows.append(row)\n",
    "    grid = [score for score in [column for column in zip(*rows)]]\n",
    "    print(*x_name, sep='\\t', file=outfile)\n",
    "    for i, row in enumerate(grid):\n",
    "        print(y_name[i], *map(str, row), sep='\\t', file=outfile)\n",
    "        \n",
    "        \n",
    "for input_file in glob.iglob(\"db_results/SpaCy_prediction/prediction_BERT/*\"):\n",
    "    '''Passez en boucle dans le dossier \"Prédictions\"'''\n",
    "    st_annotation = []\n",
    "    gold_annotation = []\n",
    "    # load the predictions and gold standard tags\n",
    "    with open(input_file, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                #print(line)\n",
    "                token, automatique, gold, validity = line.split('\\t') \n",
    "                #print(token)\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            automatique = automatique.upper()\n",
    "            # begin formatting the tags in the 'conll' format\n",
    "            st_annotation.append(f\"{token}\\t{automatique}\")\n",
    "            gold_annotation.append(f\"{token}\\t{gold}\")\n",
    "    #finish 'conll' format\n",
    "    true = '\\n'.join(gold_annotation)\n",
    "    pred_st = '\\n'.join(st_annotation)\n",
    "    #print(true)\n",
    "    #print(pred_st)\n",
    "   \n",
    "    # OUTPUT\n",
    "    # file_name to not loose the orginal information\n",
    "    #output_file = \"db_results/precision_rappel/LDSI_21_Celenza_precision_rappel.csv\"\n",
    "\n",
    "\n",
    "    # generate precision and recall report\n",
    "    evaluator = Evaluator(true, pred_st, tags=['LOC', 'PER'], loader=\"conll\")\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    "        \n",
    "        \n",
    "    # We can automatize it to do better\n",
    "    # directory out\n",
    "    output_dir = \"db_results/SpaCy_precision_rappel/precision_rappel_BERT/\"\n",
    "    # new files out with original's name plus _text and its new format .txt\n",
    "    results_file = \"%s%s_precision_rappel.csv\"%(output_dir, os.path.splitext(os.path.basename(input_file))[0])\n",
    "    print(results_file)\n",
    "    # save it as blabla_text.csv\n",
    "    with open(results_file, 'w', encoding=\"utf-8\") as fpout: \n",
    "        pretty_print(results, outfile=fpout)\n",
    "        #print to verify the result\n",
    "        #print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323b527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
