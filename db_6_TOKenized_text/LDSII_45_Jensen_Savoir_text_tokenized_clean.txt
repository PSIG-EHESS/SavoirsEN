Image
expérimentale
(à
gauche)
et
image
tirée
des
simulations
(à
droite)
des
dépôts
de
nano-agrégats
sur
une
surface
de
graphite
cristallin
(Bardotti
et
al.,
1995).
Les
deux
images
ci-dessus
illustrent
ce
qu’est
un
savoir
pour
les
physiciens
:
la
convergence
entre
une
expérience
(image
de
gauche)
et
des
simulations
numériques
issues
d’un
modèle
mathématique
(image
de
droite).
La
ressemblance
de
ces
deux
images
démontre
un
fait
considéré
jusque-là
comme
impossible
:
des
particules
contenant
des
milliers
d’atomes
peuvent
bouger
très
rapidement
sur
une
surface.
La
création
d’un
savoir
est
attestée
par
la
publication
de
ce
résultat
dans
Physical
Review
Letters
Bardotti
et
al.,
1995.
,
la
plus
prestigieuse
revue
de
physique.
Dans
le
présent
texte,
je
raconte
comment
s’est
faite,
concrètement,
la
convergence
de
ces
images,
que
l’on
pourrait
presque
confondre
tellement
elles
sont
semblables,
tellement
le
monde
artificiel
créé
par
l’ordinateur
a
été
rendu
similaire
au
monde
naturel
des
expériences,
à
moins
que
ce
ne
soit
le
contraire.
En
janvier
1993,
je
partis
passer
une
année
de
post-doctorat
à
la
Mecque
intellectuelle
des
physiciens
:
les
États-Unis
.
Il
s’agissait
d’apprendre
la
modélisation
numérique
–
jusque-là
peu
utilisée
en
France
–,
dans
un
environnement
scientifique
stimulant
:
l’équipe
de
Gene
Stanley
à
l’
université
de
Boston
.
Dans
mes
bagages,
j’emportais
de
mon
laboratoire
lyonnais
une
énigme
que
j’espérais
résoudre
grâce
à
cette
collaboration.
Il
s’agissait
d’une
contradiction
flagrante
entre
certaines
expériences
et
les
modèles
que
nous
développions
pour
comprendre
les
dépôts
de
nano-agrégats
sur
des
surfaces,
thème
central
de
nos
recherches.
Par
«
nano-agrégat
»,
on
entend
une
petite
particule
sphérique
composée
de
quelques
centaines
ou
milliers
d’atomes,
le
préfixe
nano
indiquant
que
ces
agrégats
ont
une
taille
de
l’ordre
du
nanomètre,
soit
un
milliardième
de
mètre.
De
l’importance
des
nano-agrégats
Pour
comprendre
l’importance
de
l’énigme,
il
faut
replacer
nos
études
dans
leur
contexte
scientifique
et
technologique.
Le
dépôt
de
nano-agrégats
sur
une
surface
se
situe
à
la
confluence
de
deux
questionnements
majeurs
en
physique
de
la
matière.
La
première
problématique
est
fondamentale.
Elle
concerne
la
transition
entre
les
propriétés
d’un
atome
isolé
et
celles
du
matériau
massif
correspondant.
Les
agrégats
représentent
en
effet
un
état
intermédiaire
entre
ces
deux
limites,
et
leurs
propriétés
s’avèrent
surprenantes,
ne
se
réduisant
pas
à
une
simple
interpolation
entre
les
propriétés
de
ces
extrêmes.
C’est
un
peu
comme
les
caractéristiques
des
petits
groupes
d’individus
qui
ne
se
ramènent
pas
simplement
à
celles
d’un
individu
ou
à
celles
de
la
société
tout
entière.
Ainsi,
la
couleur
des
nano-agrégats
de
certains
matériaux
dépend
fortement
de
leur
taille
:
imaginez
qu’en
cassant
une
bille
rouge
on
en
obtienne
deux
bleues…
Autre
surprise,
la
température
de
fusion
de
ces
particules
dépend
de
leur
taille
précise,
variant
de
plusieurs
dizaines
de
degrés
lorsque
l’on
ajoute
quelques
atomes.
Depuis
les
années
1960,
diverses
techniques
furent
développées
pour
fabriquer
de
tels
nano-agrégats
et
les
analyser,
de
préférence
sans
les
déposer
sur
une
surface,
car
cela
perturbe
leurs
propriétés.
L’autre
grand
questionnement,
le
dépôt
de
particules
sur
une
surface,
est
important
du
point
de
vue
technologique.
En
effet,
la
compréhension
des
processus
qui
permettent
de
réaliser
des
couches
très
minces
(quelques
milliers
d’atomes
d’épaisseur)
mais
parfaitement
contrôlées
est
à
la
base
de
toute
l’industrie
électronique.
Le
fonctionnement
de
ces
objets
quelque
peu
magiques
repose
en
effet
depuis
la
Seconde
Guerre
mondiale
sur
le
même
dispositif
:
le
transistor,
sorte
d’interrupteur
qui
permet
d’effectuer
des
calculs
élémentaires.
La
révolution
électronique
est
avant
tout
le
résultat
de
la
miniaturisation
progressive
de
ce
composant,
dont
la
taille
a
été
divisée
par
cent
mille
et
atteint
aujourd’hui
la
centaine
de
nanomètres
grâce
à
la
maîtrise
de
la
matière
à
ces
échelles
infimes
Pour
plus
de
détails,
le
lecteur
pourra
consulter
Jensen,
1996.
.
Désormais,
les
chercheurs
savent
que
cet
accroissement
de
la
puissance
des
ordinateurs
sera
bientôt
confronté
à
une
limite
physique.
En
effet,
d’ici
dix
ans
environ
les
transistors
auront
atteint
une
taille
de
quelques
dizaines
de
nanomètres
;
de
nouveaux
phénomènes
physiques
apparaîtront
alors,
empêchant
leur
bon
fonctionnement.
Ainsi,
les
courants
de
fuite
à
travers
des
barrières
isolantes
trop
fines
empêcheront
leur
fonctionnement
comme
interrupteur.
D’où
l’importance
de
trouver
de
nouveaux
dispositifs
qui
remplaceront
les
transistors,
ce
qui
passe
par
une
compréhension
des
propriétés
de
la
matière
à
l’échelle
du
nanomètre.
Nos
travaux
étaient
donc
importants
du
point
de
vue
fondamental
(la
compréhension
des
propriétés
étranges
de
la
matière
à
l’échelle
nanométrique)
et
potentiellement
riches
d’applications
technologiques
(pour
l’industrie
électronique).
Cela
permit
au
Département
de
physique
des
matériaux
Rebaptisé
depuis
la
mode
des
nanosciences
en
«
Laboratoire
de
physique
de
la
matière
condensée
et
des
nanostructures
».
de
bénéficier
de
moyens
financiers
et
humains
pour
avancer
ses
recherches,
y
compris
l’attribution
d’un
poste
de
chercheur
du
CNRS
que
j’eus
la
chance
d’occuper
dès
septembre
1990.
Les
agrégats
et
la
neige
L’objectif
général
de
mon
équipe
consistait
à
comprendre
comment
se
forme
une
couche
de
nano-agrégats.
Une
première
idée
simple
est
de
comparer
sa
croissance
à
la
formation
d’une
couche
de
neige
sur
un
trottoir
un
jour
d’hiver.
En
gros,
les
agrégats
arrivent
sur
la
surface
et
s’entassent
les
uns
sur
les
autres
comme
le
font
les
flocons
de
neige.
Mais
on
peut
se
demander
si
ce
modèle
est
correct.
Sachant
que
les
observations
directes
de
la
formation
de
la
couche
d’agrégats
sont
impossibles,
il
faut
trouver
des
moyens
indirects
pour
observer
la
croissance.
Un
premier
moyen,
adapté
au
dépôt
d’agrégats
métalliques,
consiste
à
les
déposer
sur
une
surface
de
verre
(qui
ne
conduit
pas
le
courant)
tout
en
mesurant
le
courant
qui
passe
entre
deux
électrodes
préalablement
disposées
aux
deux
bouts
de
la
plaque
de
verre.
Pendant
un
bon
moment,
aucun
courant
ne
peut
traverser
la
surface
isolante
du
verre,
mais
après
un
certain
temps
on
observe
un
saut
de
courant,
attestant
la
création
d’un
premier
chemin
métallique
entre
les
électrodes,
qui
permet
au
courant
de
circuler.
Revenons
à
notre
image
de
la
neige.
Imaginez
qu’une
fourmi
veuille
traverser
la
rue
sans
toucher
le
bitume,
en
ne
marchant
que
sur
la
neige
:
quelle
fraction
de
la
rue
faut-il
recouvrir
de
neige
pour
qu’elle
puisse
passer
d’un
trottoir
à
l’autre
?
Bien
sûr,
si
la
rue
est
totalement
recouverte,
c’est
possible,
mais
un
modèle
mathématique
simple,
dit
de
«
percolation
De
Gennes,
1976.
»,
montre
qu’il
existe
un
chemin
continu
reliant
les
deux
trottoirs
dès
que
59
%
de
la
rue
a
été
recouverte.
En
partant
de
ce
modèle
simple,
on
peut
calculer
la
quantité
d’agrégats
qu’il
faut
accumuler
sur
la
surface
pour
observer
le
saut
de
courant
(le
«
seuil
de
conduction
»).
À
notre
agréable
surprise,
ce
calcul
prédit
assez
bien
le
seuil
de
conduction
et
permet
également
de
comprendre
la
raison
pour
laquelle
cette
quantité
dépend
de
la
taille
des
agrégats.
Comme
il
est
plutôt
rare
qu’un
modèle
aussi
simple
puisse
expliquer
des
phénomènes
réels,
nous
nous
sommes
empressés
de
publier
notre
modèle
Jensen
et
al.,
1992.
.
Hélas,
d’autres
expériences
n’ont
pas
tardé
à
nous
montrer
ses
limites
en
établissant
que
le
seuil
de
conduction
dépend
fortement
du
flux
incident
d’agrégats,
contrairement
à
la
constance
prédite
par
le
modèle
simple.
En
effet,
le
nombre
de
flocons
de
neige
nécessaires
pour
recouvrir
59
%
d’une
surface
ne
dépend
pas
de
l’intensité
de
l’averse
de
neige
;
celle-ci
change
simplement
le
temps
nécessaire
pour
parvenir
à
cette
couverture.
Ce
fait
expérimental
démontrait
que
quelque
chose
clochait
dans
la
formule,
qu’un
phénomène
autre
qu’un
simple
empilement
des
agrégats
intervenait.
Lequel
?
Incapables
d’observer
les
agrégats
à
l’œuvre,
nous
en
étions
réduits
à
des
hypothèses
:
les
agrégats
pouvaient
bouger
sur
la
surface
une
fois
déposés
(ce
qui
n’arrive
pas
aux
flocons
!),
ils
pouvaient
s’oxyder
progressivement
pendant
le
dépôt
(la
couche
d’oxyde
empêchant
le
passage
du
courant),
ou
bien
ils
pouvaient
coalescer
sur
la
surface,
comme
deux
gouttes
d’eau
qui
fusionnent
pour
n’en
donner
qu’une,
plus
grosse.
Tous
ces
facteurs
étaient
susceptibles
en
principe
d’affecter
le
seuil
de
conduction
lorsque
le
flux
incident
variait.
Mais
lequel
ou
lesquels
étaient
réellement
à
l’œuvre
dans
les
expériences
?
Nous
n’en
savions
rien.
Une
chambre
de
dépôt
virtuelle
Voilà
le
mystère
que
je
voulais
élucider
lors
de
mon
année
de
post-doctorat
à
Boston
.
Après
quelques
discussions
avec
mes
nouveaux
collègues,
je
décide
d’explorer
l’hypothèse
de
la
diffusion
des
agrégats.
Les
raisons
de
ce
choix
sont
assez
subjectives,
mêlant
des
influences
scientifiques
et
culturelles.
D’abord,
la
diffusion
est
relativement
facile
à
programmer.
Elle
fait
donc
partie
de
la
culture
du
milieu,
car
des
modèles
antérieurs
l’ont
prise
en
compte
dans
d’autres
contextes.
Ensuite,
il
existe
depuis
le
début
du
xx
e
siècle
une
théorie
solide
et
simple
de
la
diffusion
des
particules
sur
une
surface.
En
revanche,
les
autres
hypothèses
expérimentales
(oxydation,
coalescence),
bien
que
tout
aussi
vraisemblables
du
point
de
vue
scientifique,
présentent
des
difficultés
supplémentaires
concernant
la
théorie
ou
la
programmation.
Un
scientifique
voulant
publier
doit
avancer
par
des
chemins
de
moindre
résistance…
Pourtant,
inclure
la
diffusion
des
agrégats
dans
la
modélisation
pouvait
apparaître
peu
raisonnable.
En
effet,
la
communauté
scientifique
sait
que
les
atomes
individuels
peuvent
diffuser
rapidement
sur
une
surface,
car
ils
sont
très
petits
et
donc
sensibles
au
tangage
des
surfaces,
induit
par
l’agitation
thermique
des
atomes
de
surface.
Cependant,
tous
les
experts
considéraient
impossible
la
diffusion
des
nano-agrégats.
En
effet,
ceux-ci
étant
plus
gros
que
les
atomes,
ils
peuvent
s’ancrer
fortement
à
la
surface
et
rester
immobiles.
Malgré
la
relative
simplicité
de
la
diffusion,
il
est
impossible
de
trouver
des
formules
mathématiques
donnant
le
seuil
de
conduction
lorsque
l’on
inclut
la
diffusion
des
agrégats.
D’où
l’intérêt
des
simulations
par
ordinateur,
que
le
laboratoire
de
Boston
maîtrise
parfaitement.
Il
s’agit
de
reproduire,
in
silico
,
ce
qui
se
passe
lors
du
dépôt
d’agrégats,
en
mimant
les
processus
un
à
un
et
en
suivant
leur
déroulement.
Échec
et
divine
surprise
Je
me
lance
alors
tête
baissée
dans
la
programmation
du
modèle.
Au
bout
de
quatre
mois
assez
pénibles
de
programmation,
de
bugs
et
d’erreurs
diverses,
je
parvins
enfin
à
écrire
un
code
capable
de
simuler
le
dépôt
d’agrégats
ainsi
que
leur
diffusion
sur
la
surface.
J’avais
en
quelque
sorte
créé
une
chambre
de
dépôt
virtuelle,
où
je
pouvais
effectuer
des
«
expériences
»
numériques
tout
en
observant
en
détail
comment
la
couche
d’agrégats
se
forme.
Je
pouvais
changer
à
loisir
le
flux
d’agrégats
incidents
et
observer
son
influence
sur
la
quantité
d’agrégats
nécessaire
pour
former
un
chemin
continu,
comme
dans
les
expériences
réelles.
Le
résultat
essentiel
est
simple
:
la
diffusion
des
agrégats
augmente
effectivement
le
seuil
de
conduction,
comme
nous
en
avions
l’intuition,
mais
de
manière
très
faible
!
La
conclusion
de
ces
mois
de
travail
dans
l’hiver
rigoureux
de
Boston
est
sans
appel
:
la
diffusion
des
agrégats
ne
peut
expliquer
les
résultats
expérimentaux.
Malgré
le
découragement
–
écarter
une
hypothèse
ne
conduit
pas
à
une
publication
prestigieuse
–,
j’informe
les
chercheurs
restés
à
Lyon
des
résultats
par
courrier
électronique,
qui
représentait
alors
un
nouvel
outil
de
communication
léger
et
flexible.
Cela
me
permet
notamment
de
leur
transmettre
des
images
générées
par
le
modèle,
que
je
trouve
malgré
tout
assez
jolies.
Alain
Hoareau
,
directeur
de
l’équipe
lyonnaise,
note
une
ressemblance
frappante
entre
les
images
du
modèle
et
celles
obtenues
quelques
semaines
auparavant
par
Laurent
Bardotti
,
alors
thésard
au
laboratoire.
Il
s’agissait
d’images
résultant
d’une
expérience
menée
de
manière
totalement
indépendante,
et
qui
visait
à
observer
les
couches
d’agrégats
au
microscope
à
effet
tunnel.
Ce
type
de
microscope,
à
la
mode
depuis
que
le
prix
Nobel
a
été
décerné
à
ses
inventeurs
en
1986,
permet
de
mesurer
la
hauteur
de
la
couche
d’agrégats.
Pour
mener
à
bien
ces
observations,
il
faut
déposer
les
agrégats
sur
une
surface
lisse
et
sans
défauts,
le
graphite
cristallin.
Heureux
hasard,
l’interaction
entre
l’agrégat
et
cette
surface
est
très
faible,
ce
qui
permet
à
ces
particules
de
diffuser
fortement,
et
explique
la
ressemblance
avec
les
images
issues
du
modèle.
Nous
reconnaissons
là
un
point
d’accrochage
possible
entre
des
expériences
et
un
modèle,
et
nous
y
concentrons
nos
efforts.
D’autant
plus
que,
parallèlement,
une
équipe
suisse
est
parvenue
à
obtenir
des
images
identiques
de
dépôts
atomiques,
technique
nettement
plus
répandue
que
celle
utilisée
à
Lyon
.
Notre
modèle
pourrait
donc
constituer
un
«
point
de
passage
obligé
Ce
terme
renvoie
aux
analyses
de
Bruno
Latour
et
Michel
Callon,
voir
par
exemple
Latour,
2001.
»,
non
seulement
pour
la
petite
communauté
du
dépôt
d’agrégats,
mais
également
pour
celle,
autrement
plus
importante,
du
dépôt
d’atomes.
Nous
nous
dépêchons
alors
d’oublier
la
question
initiale,
qui
ne
présente
plus
d’intérêt
en
termes
d’impact
et
donc
de
publication.
Cette
stratégie,
conjuguée
à
l’influence
du
groupe
de
Gene
Stanley
,
conduit
à
la
publication
dans
la
prestigieuse
revue
Nature
d’une
présentation
sommaire
du
modèle
Jensen
et
al.,
1994.
,
explicitement
relié
aux
expériences
sur
les
atomes.
La
présentation
détaillée
du
modèle
sera
publiée
quelque
temps
plus
tard,
dans
une
revue
de
physique
Jensen
et
al.,
1994a.
.
Notre
travail
de
modélisation
a
finalement
abouti
à
plusieurs
résultats.
D’abord,
un
modèle
valide
pour
la
croissance
des
couches
d’atomes
ou
d’agrégats.
Ce
modèle
a
été
utilisé
par
de
nombreux
groupes
dans
le
monde
pour
l’analyse
de
leurs
expériences.
Nous
l’avons
bien
sûr
beaucoup
utilisé
à
Lyon
,
pour
mieux
comprendre
les
dépôts
d’agrégats,
aboutissant
à
un
résultat
étonnant.
Contrairement
à
ce
que
l’on
croyait
jusque-là,
les
agrégats
sont
capables
de
diffuser
très
rapidement
sur
certaines
surfaces.
Ce
savoir
a
été
reconnu
par
la
communauté
scientifique,
comme
l’atteste
le
grand
nombre
de
citations
(positives)
que
l’article
a
reçues
Dix
citations
par
an
depuis
sa
publication,
chiffre
que
l’on
peut
comparer
aux
citations
reçues
en
moyenne
par
un
article
de
physique
(moins
d’une
citation
par
an)
ou
par
l’article
d’Albert
Fert
qui
lui
valut
le
prix
Nobel
en
2007
(deux
cents
citations
par
an).
.
Plusieurs
équipes
de
par
le
monde
ont
en
effet
étendu
notre
étude,
en
explorant
le
dépôt
d’agrégats
d’autres
matériaux,
en
imaginant
des
modèles
atomiques
capables
d’expliquer
la
diffusion
rapide
des
nano-agrégats
ou
leur
coalescence
sur
la
surface.
Comprendre,
pour
un
physicien…
Pour
mieux
comprendre
la
démarche
qui
nous
a
conduits
à
la
création
d’un
savoir,
il
convient
de
replacer
notre
travail
dans
l’histoire
globale
de
la
physique.
Voici
quatre
siècles,
les
physiciens
définissaient
le
savoir
légitime
comme
celui
qui
peut
se
rattacher
à
une
modélisation
rigoureuse.
Comme
le
résume
Maurice
Clavelin
dans
son
étude
sur
Galilée
Clavelin,
1996.
:
expliquer,
pour
un
physicien,
c’est
transformer
un
fait
physique
en
un
problème
mathématique,
puis
le
résoudre
grâce
aux
outils
rigoureux
de
cette
discipline.
Ce
n’est
pas
le
lieu
ici
de
discuter
de
la
fécondité
et
des
limites
globales
de
cette
démarche
intellectuelle
Je
renvoie
le
lecteur
à
Jensen,
2001.
.
Notre
exemple
incite
plutôt
à
centrer
la
discussion
autour
de
deux
thèmes
:
le
rôle
des
expériences
contrôlées
en
laboratoire
et
celui
des
simulations
numériques.
Commençons
par
les
expérimentations
contrôlées.
Pour
créer
un
savoir
légitime,
la
physique
se
limite
à
la
portion
de
réalité
qui
peut
être
connectée
à
un
modèle
mathématique
rigoureux.
Cela
passe
notamment
par
la
fabrication
d’un
monde
artificiel
purifié.
Dans
notre
exemple,
il
s’agit
d’abord
de
la
surface
cristalline
de
graphite,
qui
doit
être
très
plane
pour
ne
pas
perturber
la
diffusion
uniforme
des
agrégats,
la
seule
que
l’on
peut
décrire
simplement.
Les
dépôts
sur
du
verre
ou
sur
du
carbone
désordonné
(amorphe)
sont
d’ailleurs
toujours
mal
compris.
De
plus,
le
dépôt
doit
être
effectué
dans
un
environnement
purifié,
sous
un
vide
poussé
qui
élimine
la
contamination
des
agrégats
par
l’air
ambiant.
Pour
que
la
jonction
entre
un
modèle
et
des
expériences
puisse
s’établir,
il
faut
bien
sûr
construire
un
modèle
adapté.
Mais
le
plus
difficile
consiste
à
réaliser
des
«
manips
»
suffisamment
contrôlées
pour
que
la
réalité
ressemble
suffisamment
au
modèle.
Parmi
les
expériences
menées
dans
ce
cadre
purifié,
on
oublie
celles
qui
conduisent
à
des
questions
intéressantes,
mais
ne
peuvent
être
reliées
à
un
modèle.
Nous
avons
ainsi
laissé
de
côté
la
question
de
départ,
l’augmentation
du
seuil
de
conduction
avec
le
flux
incident
d’agrégats,
énigme
qui
n’est
toujours
pas
résolue.
Dans
les
publications,
l’effet
du
flux
est
attribué
paresseusement
à
l’oxydation
des
nano-agrégats
d’antimoine,
sans
trop
de
preuves.
Comme
l’a
noté
l’anthropologue
des
sciences
Pascal
Lécaille
lors
d’un
stage
dans
notre
laboratoire
Lécaille,
1996.
,
Laurent
Bardotti
s’aidait
de
deux
classeurs
–
étiquetés
«
utile
»
et
«
poubelle
»
–
pour
ranger
les
résultats
de
ses
expériences.
Le
premier
permet
de
préclasser
ce
qui
a
une
chance
d’être
comparé
et
amarré
aux
simulations,
le
deuxième
ce
qui
selon
toute
vraisemblance
restera
en
attente
et
finira
dans
une
poubelle
à
la
fin
de
la
thèse.
Enfin,
pour
réussir
cette
jonction,
il
existe
des
«
tours
de
main
»
précis,
acquis
localement
par
l’expérience,
le
contact
quotidien
avec
les
instruments
ou
les
ordinateurs
Collins,
2001.
.
Il
a
ainsi
fallu
un
temps
de
tâtonnements
à
Laurent
Bardotti
avant
qu’il
réussisse
à
obtenir
et
à
reproduire
des
surfaces
de
graphite
suffisamment
lisses.
Concrètement,
il
y
est
parvenu
en
les
chauffant
à
500
degrés
pendant
quelques
heures
–
pour
éliminer
les
impuretés
chimiques
–
et
en
les
pelant
grâce
à
du
scotch
collé
à
leur
surface
–
pour
obtenir
une
surface
suffisamment
lisse.
Abordons
à
présent
le
deuxième
moyen
dont
disposent
les
physiciens
pour
relier
le
monde
réel
et
les
mathématiques.
Les
simulations
représentent
aujourd’hui
un
moyen
commode
d’extension
du
monde
rigoureux
des
mathématiques
cher
à
Galilée
.
Elles
présentent
des
similitudes
avec
les
mathématiques,
qui
les
rendent
suffisamment
rigoureuses,
et
en
même
temps
des
différences,
qui
les
rendent
plus
intéressantes
dans
de
nombreux
contextes
Sur
la
nouveauté
apportée
par
les
simulations,
notamment
dans
le
domaine
de
la
fiabilité
et
de
la
preuve
mathématique,
la
lecture
indispensable
est
MacKenzie,
2001.
.
Elles
aident
donc
à
établir
un
pont
entre
réalité
et
mathématiques
de
plusieurs
manières.
D’abord,
bien
sûr,
en
construisant,
à
l’égal
des
mathématiques,
un
monde
bien
défini,
rigoureux,
sans
(trop
de)
surprises
une
fois
qu’il
a
été
défini
et
exploré.
Ainsi,
le
modèle
utilisé
ici
a
permis
d’interpréter
les
expériences
en
les
reliant
à
des
bases
physiques
solides
(la
diffusion
des
agrégats
notamment)
de
manière
aussi
sûre
qu’une
théorie
mathématique.
Pour
preuve,
notre
interprétation
a
tenu
face
aux
objections
de
nos
collègues
qui
doutaient
de
notre
résultat
improbable
:
les
agrégats
se
déplacent
des
millions
de
fois
plus
vite
que
ce
qui
était
accepté
jusque-là.
Ensuite,
autre
similarité
avec
les
mathématiques,
les
simulations
nous
obligent
(et
nous
aident)
à
transformer
nos
intuitions
en
des
règles
rigoureuses,
mécaniques.
On
peut
évoquer
trois
manières
différentes
pour
parvenir
à
ce
but
:
en
forçant
le
programme
à
être
cohérent
(via
la
compilation)
;
en
explorant
tous
les
cas
possibles
(qui
finissent
par
arriver
lors
des
simulations
Comme
la
rencontre
entre
trois
particules
qui
diffusent,
événement
rare
que
je
n’avais
pas
prévu
explicitement
et
qui
a
«
planté
»
la
première
version
du
programme.
)
;
en
obligeant
à
préciser
les
valeurs
de
tous
les
paramètres
qu’on
a
introduits
mais
sur
lesquels
on
n’a
pas
forcément
envie
d’être
précis.
Écrire
un
programme
opérationnel,
c’est
dialoguer
avec
un
interlocuteur
implacable,
qui
oblige
à
préciser
sa
pensée,
force
à
décider
ce
que
le
modèle
indique
dans
chaque
configuration,
sans
ambiguïté
Pour
une
discussion
plus
générale
du
rôle
similaire
de
l’écriture
par
rapport
à
l’oral,
on
se
référera
au
livre
classique
de
Goody,
1979.
.
Les
simulations
présentent
néanmoins
des
différences
importantes
avec
l’approche
mathématique
habituelle,
qui
vise
à
établir
des
formules
analytiques
générales.
Les
simulations
nous
permettent
en
effet
de
dialoguer
avec
le
monde
mathématique
de
manière
plus
flexible,
en
utilisant
d’autres
variables,
d’autres
degrés
de
liberté
plus
proches
de
l’intuition,
plus
proches
aussi
de
ceux
utilisés
dans
les
expériences
Pour
une
discussion
approfondie
de
l’importance
croissante
des
simulations
dans
les
sciences,
le
lecteur
pourra
consulter
Galison,
1997,
Creager
et
Lunbeck,
2007.
Sur
le
cas
spécifique
de
la
physique
de
la
matière,
voir
Jensen
et
Blase,
2002.
.
Ainsi
pour
modéliser
la
rencontre
entre
les
agrégats
qui
diffusent
sur
une
surface
et
des
îlots
déjà
formés,
les
mathématiques
doivent
passer
par
des
coefficients
(les
«
sections
efficaces
de
capture
»)
qui
tentent
de
rendre
compte,
en
moyenne,
de
leur
probabilité
de
rencontre.
Ces
coefficients
n’ont
pu
être
calculés,
de
manière
approchée,
que
pour
des
îlots
supposés
circulaires,
alors
que
les
formes
réelles
sont
plus
complexes.
Et
tandis
que
les
simulations
obtiennent
sans
peine
ces
formes
fractales
(comme
celles
des
illustrations
plus
haut),
les
formules
mathématiques
ne
peuvent
rendre
compte
que
d’îlots
circulaires.
Or
il
est
clair
que
les
images
obtenues
grâce
aux
simulations
permettent
une
confrontation
plus
aisée
avec
les
expériences.
Dans
notre
exemple,
elles
ont
même
donné
l’impulsion
initiale
à
la
connexion
entre
expériences
et
modèle,
qui
ne
se
serait
sans
doute
pas
faite
autrement…
Un
modélisateur
est
confronté
en
permanence
à
deux
partenaires
inflexibles
:
l’expérimentateur
qui
lui
rappelle
que
son
modèle
ne
doit
pas
trop
s’éloigner
du
monde
expérimental,
dont
il
se
fait
le
porte-parole,
et
l’ordinateur
qui
l’oblige
à
rester
rigoureux,
précis.
Cette
co-construction
du
modèle
est
essentielle
pour
que
soit
réussi
l’alignement
entre
mathématiques
et
expériences.
D’où
l’importance
de
contacts
quotidiens
entre
expérimentateurs
et
théoriciens,
induisant
une
meilleure
adaptation
réciproque.
Dans
ce
contexte,
l’article
scientifique
joue
un
rôle
important
de
cristallisateur
des
tâtonnements
quotidiens,
en
forçant
l’accord
sur
certains
résultats,
obtenus
au
jour
le
jour,
dans
un
foisonnement
d’avis
et
de
contacts.
Épilogue
:
Simuler
la
société
?
Ce
travail
m’avait
enseigné
la
puissance
des
simulations
numériques,
capables
de
dépasser
les
mathématiques
dans
la
compréhension
de
la
croissance
des
couches
de
nano-agrégats.
Après
une
dizaine
d’années
consacrées
à
la
modélisation
des
nano-agrégats,
j’ai
eu
envie
de
changer
de
domaine
et
de
tester
cette
approche
sur
un
champ
beaucoup
plus
difficile
:
les
sciences
sociales.
Envisagée
comme
une
méthode
de
compréhension
générale
du
social,
cette
idée
peut
sembler
au
mieux
naïve,
au
pis
irresponsable.
En
effet,
l’étude
évoquée
précédemment
nous
a
montré
l’importance
de
l’expérimentation
contrôlée,
des
situations
purifiées
pour
parvenir
à
la
compréhension
rigoureuse
au
sens
du
physicien.
Quelle
pertinence
espérer
pour
cette
approche
dans
le
domaine
social,
heureusement
peu
contrôlable
?
Côté
simulations,
s’il
est
tentant
de
fabriquer
des
«
sociétés
virtuelles
»
in
silico
,
en
partant
du
comportement
des
individus,
il
est
clair
que
cet
«
individualisme
méthodologique
»
est
par
trop
simplificateur.
Il
ignore
par
exemple
un
fait
essentiel
sur
le
plan
sociologique
:
les
relations
«
sociales
»
sont
durables,
car
elles
sont
portées
également
par
des
objets,
des
institutions,
c’est-à-dire
des
structures
au
niveau
mésoscopique
qui
ne
sont
pas
prises
en
compte
dans
cette
approche
Comme
le
démontre
avec
humour
Bruno
Latour
(Latour,
2007),
l’individualisme
méthodologique
est
une
approche
parfaitement
adaptée
à
une
société
de
babouins,
dont
les
structures
sociales
sont
très
limitées
et
coûteuses
(en
temps)
à
entretenir
!
.
À
quel
type
de
savoir
peut-on
envisager
de
parvenir
en
simulant
«
rigoureusement
»
le
social
Pour
une
analyse
subtile
de
la
«
vanité
de
la
rigueur
en
économie
»,
voir
Cartwright,
2005.
?
Je
n’ai
bien
entendu
pas
de
réponse
définitive
à
cette
question.
Cependant,
il
existe
des
éléments
objectifs
qui
poussent
à
s’intéresser
à
des
systèmes
sociaux
spécifiques
armés
des
outils
de
modélisation
du
physicien.
D’abord,
depuis
quelques
années,
des
données
numériques
de
plus
en
plus
nombreuses
deviennent
disponibles.
Citons
les
itinéraires
de
voitures
suivies
par
GPS,
les
réseaux
scientifiques
décrits
par
les
articles
et
leurs
citations,
les
transactions
sur
Internet
ou
encore
la
géolocalisation
des
appels
téléphoniques
par
les
portables.
Face
à
cette
avalanche
de
données,
des
outils
d’analyse
quantitatifs
soutenus
par
la
puissance
des
ordinateurs
peuvent
aider
à
mieux
comprendre
les
mécanismes
sociaux
sous-jacents.
Ainsi,
la
jeune
science
des
réseaux
complexes
Voir,
par
exemple,
une
présentation
flatteuse
dans
Barabási
et
Bonabeau,
2003.
Pour
une
analyse
plus
critique,
on
consultera
Keller,
2005.
permet
de
fouiller
des
millions
d’articles
et
d’éclairer
l’histoire
des
disciplines
scientifiques
Pour
l’exemple
de
l’émergence
d’une
discipline
à
l’interface
entre
biologie
et
cancer,
voir
Cambrosio
et
al.,
2006.
,
et
cela
de
manière
complémentaire
à
l’approche
traditionnelle
par
l’analyse
approfondie
de
quelques
textes.
Ce
n’est
d’ailleurs
pas
la
première
fois
que
l’avalanche
de
données
sociales
suscite
des
tentations
du
côté
des
chercheurs
en
sciences
exactes.
Ainsi,
au
début
du
xix
e
siècle,
l’affirmation
des
bureaucraties
étatiques
avait
conduit
à
la
collecte
de
nombreuses
données
à
l’échelle
des
nations
(taux
de
naissance,
de
mortalité,
de
suicide…).
Ces
données
ont
révélé
des
régularités
inattendues,
faisant
fantasmer
des
astronomes
comme
Quételet
qui
conçut
l’idée
d’une
«
physique
sociale
».
Le
bilan
de
ce
mariage
entre
sciences
sociales
et
mathématiques
est
fort
mince
du
côté
des
sciences
sociales.
Il
est
bien
plus
intéressant
pour
les
mathématiques
appliquées
et
la
physique,
qui
ont
hérité
d’un
nouvel
outil
d’analyse
de
la
variabilité
du
réel,
la
loi
Normale
Pour
une
histoire
de
ces
échanges
interdisciplinaires,
le
lecteur
pourra
consulter
Desrosières,
2002,
ainsi
que
Porter,
1994.
.
L’ordinateur
permet
également
de
concevoir
des
sociétés
virtuelles
in
silico
.
Le
but
consiste
à
expliquer
le
niveau
macroscopique
(la
société)
par
le
niveau
microscopique
(les
individus
et
leurs
interactions).
En
économie,
avec
ce
type
de
simulations
on
peut
dépasser
un
grand
nombre
d’hypothèses
restrictives
adoptées
pour
simplifier
les
calculs,
menant
à
cet
individu
caricatural
qu’est
l’
homo
œconomicus
.
Les
simulations
de
type
«
multi-agents
»
permettent
en
effet
d’analyser
des
systèmes
comprenant
des
individus
hétérogènes,
à
rationalité
limitée
et
n’ayant
qu’une
connaissance
imparfaite
du
monde.
Pourtant,
si
les
physiciens
acceptent
couramment
que
les
simulations
prolongent
la
rigueur
mathématique,
cela
n’est
pas
(encore)
le
cas
des
économistes,
qui
continuent
à
privilégier
les
démonstrations
mathématiques.
Le
physicien
qui
s’y
risque
doit
passer
par
un
long
travail
d’acculturation
pour
rentrer
dans
le
paradigme
économique.
Cela
se
traduit
notamment
par
l’utilisation
des
caractéristiques
de
base
des
acteurs
économiques
(utilité
individuelle,
prix
de
réserve,
stratégie
d’optimisation…)
qui
rendent
le
savoir
produit
par
les
simulations
assimilable
par
les
économistes
et
cumulable
avec
leurs
modèles.
Cela
est
souvent
frustrant
pour
le
physicien,
qui
trouve
que
ces
caractéristiques
sont
plus
dictées
par
convenance
mathématique
que
par
des
données
empiriques.
Mais,
à
moins
de
vouloir
refonder
l’économie
sur
de
nouveaux
ingrédients
de
base,
les
simulations
doivent
se
plier
à
ceux
utilisés
aujourd’hui,
faute
de
quoi
les
résultats
ne
seront
pas
considérés
comme
un
savoir…
en
économie.
Bibliographie
