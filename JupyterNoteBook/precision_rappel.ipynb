{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204148bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/BNU_01_Didier_textSpaCy_bert_prediction.tsv\n",
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t50\t47\t45\t47\n",
      "incorrect\t6\t0\t11\t9\n",
      "partial\t0\t9\t0\t0\n",
      "missed\t3\t3\t3\t3\n",
      "spurious\t31\t31\t31\t31\n",
      "possible\t59\t59\t59\t59\n",
      "actual\t87\t87\t87\t87\n",
      "precision\t0.575\t0.592\t0.517\t0.54\n",
      "recall\t0.847\t0.873\t0.763\t0.797\n",
      "f1\t0.685\t0.705\t0.616\t0.644\n",
      "None\n",
      "/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/BNU_02_Colin_textSpaCy_bert_prediction.tsv\n",
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t75\t44\t44\t44\n",
      "incorrect\t0\t0\t31\t31\n",
      "partial\t0\t31\t0\t0\n",
      "missed\t177\t177\t177\t177\n",
      "spurious\t460\t460\t460\t460\n",
      "possible\t252\t252\t252\t252\n",
      "actual\t535\t535\t535\t535\n",
      "precision\t0.14\t0.111\t0.082\t0.082\n",
      "recall\t0.298\t0.236\t0.175\t0.175\n",
      "f1\t0.191\t0.151\t0.112\t0.112\n",
      "None\n",
      "/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/BNU_03_Jacob_textSpaCy_bert_prediction.tsv\n",
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t42\t43\t41\t43\n",
      "incorrect\t3\t0\t4\t2\n",
      "partial\t0\t2\t0\t0\n",
      "missed\t1\t1\t1\t1\n",
      "spurious\t46\t46\t46\t46\n",
      "possible\t46\t46\t46\t46\n",
      "actual\t91\t91\t91\t91\n",
      "precision\t0.462\t0.484\t0.451\t0.473\n",
      "recall\t0.913\t0.957\t0.891\t0.935\n",
      "f1\t0.613\t0.642\t0.599\t0.628\n",
      "None\n",
      "/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/BNU_04_Gfrereis_textSpaCy_bert_prediction.tsv\n",
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t3\t3\t3\t3\n",
      "incorrect\t0\t0\t0\t0\n",
      "partial\t0\t0\t0\t0\n",
      "missed\t0\t0\t0\t0\n",
      "spurious\t59\t59\t59\t59\n",
      "possible\t3\t3\t3\t3\n",
      "actual\t62\t62\t62\t62\n",
      "precision\t0.048\t0.048\t0.048\t0.048\n",
      "recall\t1.0\t1.0\t1.0\t1.0\n",
      "f1\t0.092\t0.092\t0.092\t0.092\n",
      "None\n",
      "/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/BNU_05_Dege_textSpaCy_bert_prediction.tsv\n",
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t10\t0\t0\t0\n",
      "incorrect\t0\t0\t10\t10\n",
      "partial\t0\t10\t0\t0\n",
      "missed\t26\t26\t26\t26\n",
      "spurious\t62\t62\t62\t62\n",
      "possible\t36\t36\t36\t36\n",
      "actual\t72\t72\t72\t72\n",
      "precision\t0.139\t0.069\t0.0\t0.0\n",
      "recall\t0.278\t0.139\t0.0\t0.0\n",
      "f1\t0.185\t0.093\t0\t0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from nervaluate import Evaluator\n",
    "import argparse\n",
    "\n",
    "'''\n",
    "Ce script utilise nervaluate pour calculer la précision et le rappel\n",
    "du marquage automatique des entités nommées par rapport à un étalon-or. \n",
    "Il recherche un file \"prediction\", qui doit contenir toutes les informations à évaluer sous la forme suivante:\n",
    "TOKEN   PREDICTION     GOLD   VALIDITY\n",
    "Deux       O             O        1\n",
    "mois       O             O        1\n",
    "...\n",
    "Les scores sont générés pour chaque fichier et enregistrés \"precision_rappel\".\n",
    "'''\n",
    "\n",
    "# Initialize the parser\n",
    "parser = argparse.ArgumentParser\n",
    "\n",
    "def pretty_print(result, outfile=None):\n",
    "    \"\"\"display the result\"\"\"\n",
    "    x_name = ['Measure'] + [k for k in result]\n",
    "    y_name = []\n",
    "    rows = []\n",
    "    for evaluation in result:\n",
    "        metrics = result[evaluation]\n",
    "        row = []\n",
    "        for metric, score in metrics.items():\n",
    "            if metric not in y_name:\n",
    "                y_name.append(metric)\n",
    "            row.append(round(score, 3)) # Arrondir\n",
    "        rows.append(row)\n",
    "    grid = [score for score in [column for column in zip(*rows)]]\n",
    "    print(*x_name, sep='\\t', file=outfile)\n",
    "    for i, row in enumerate(grid):\n",
    "        print(y_name[i], *map(str, row), sep='\\t', file=outfile)\n",
    "        \n",
    "# Insert the path        \n",
    "for input_file in glob.iglob(\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_db/*\"):\n",
    "    '''loop through the directory'''\n",
    "    st_annotation = []\n",
    "    gold_annotation = []\n",
    "    # load the predictions and gold standard tags\n",
    "    with open(input_file, 'r', encoding='utf-8') as fin:\n",
    "        print(input_file)\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                #print(line)\n",
    "                token, automatique, gold, validity = line.split('\\t') \n",
    "                #print(token)\n",
    "            except ValueError:\n",
    "                print(line)\n",
    "            automatique = automatique.upper()\n",
    "            # begin formatting the tags in the 'conll' format\n",
    "            st_annotation.append(f\"{token}\\t{automatique}\")\n",
    "            gold_annotation.append(f\"{token}\\t{gold}\")\n",
    "    #finish 'conll' format\n",
    "    true = '\\n'.join(gold_annotation)\n",
    "    pred_st = '\\n'.join(st_annotation)\n",
    "    #print(true)\n",
    "    #print(pred_st)\n",
    "   \n",
    "    # generate precision and recall report\n",
    "    evaluator = Evaluator(true, pred_st, tags=['LOC', 'PER'], loader=\"conll\")\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    "    print(pretty_print(results))\n",
    "    \n",
    "        \n",
    "    \"\"\"    \n",
    "    # We can automatize it to do better\n",
    "    # directory out\n",
    "    output_dir = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/test_result/\"\n",
    "    # new files out with original's name plus _text and its new format .txt\n",
    "    results_file = \"%s%s_precision_rappel.csv\"%(output_dir, os.path.splitext(os.path.basename(input_file))[0])\n",
    "    print(results_file)\n",
    "    # save it as blabla_text.csv\n",
    "    with open(results_file, 'w', encoding=\"utf-8\") as fpout: \n",
    "        pretty_print(results_by_tag, outfile=fpout)\n",
    "        #print to verify the result\n",
    "        #print(first)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbe8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expansion of jupyter notebook\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a6d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
