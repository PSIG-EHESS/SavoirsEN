{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d22694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fda1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "#!cat *.tsv> eval_data_test.tsv\n",
    "\n",
    "#   \n",
    "#!less eval_data_test.tsv\n",
    "#\n",
    "#!wc -l eval_data_test.tsv\n",
    "#\n",
    "#!wc -l *.tsv\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f7fa1",
   "metadata": {},
   "source": [
    "## ici, c'est une zone markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ici une cellule d'ex√©cution des scrpits\" \n",
    "\n",
    "# open the file\n",
    "with open (\"/Savoirs_env/db_1_savoirs_XMLtei/BNU_02_Colin.xml\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1b680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bda260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "with open (\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_2_NERbeyond/BNU_01_Didier_text_beyond.xml\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01068e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f68f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "with open (\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_4_OutputNerBeyond/BNU_01_Didier_text_beyond.conll\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ec4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# regular expression\n",
    "import re\n",
    "# retrives file support\n",
    "import glob\n",
    "# inform the path\n",
    "\n",
    "# replacement from NER & Beyond to IOB format    \n",
    "_replacements = {\n",
    "            'B-TOP_GR': 'B-LOC',\n",
    "            'B-PERSNAME_NAME': 'B-PER',\n",
    "            'I-PERSNAME_NAME': 'I-PER',\n",
    "            'I-TOP_GR': 'I-LOC',\n",
    "            }\n",
    "\n",
    "def _do_replace(text):\n",
    "    return _replacements.get(text.group(0))\n",
    "\n",
    "def replace_tags(text, _re=re.compile('|'.join(re.escape(r) for r in _replacements))):\n",
    "        return _re.sub(_do_replace, text)\n",
    "\n",
    "for file_path in glob.iglob('/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_4_OutputNerBeyond/*.conll'):\n",
    "    #transform path into a readable file\n",
    "    f_name = (file_path)\n",
    "    #print(f_name)\n",
    "    \n",
    "    # open the variable to be read and split into words\n",
    "    with open(f_name, 'r', encoding='utf8') as f:\n",
    "        # read and split into words to count word in file\n",
    "        text = f.read()\n",
    "        #print(text)\n",
    "    \n",
    "    result = (replace_tags(text))\n",
    "    print(result)\n",
    "\"\"\"\n",
    "# directory out\n",
    "    output_dir = \"db_BIO/\"\n",
    "    # new files out with original's name plus _text and its new format .txt\n",
    "    results_file = \"%s%s_BIO.conll\"%(output_dir, os.path.splitext(os.path.basename(f_name))[0])\n",
    "    print(results_file)\n",
    "    # save it as blabla_BIO.CONLL\n",
    "    with open(results_file, 'w') as fpout: \n",
    "        fpout.write(str(result))\n",
    "        #print to verify the result\n",
    "        #print(first)\n",
    "  \"\"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95684c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# regular expression\n",
    "import re\n",
    "# retrives file support\n",
    "import glob\n",
    "# make a table\n",
    "import pandas as pd\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "# inform the path\n",
    "for file_path in glob.iglob('/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_4_OutputNerBeyond/*.conll'):\n",
    "    #transform path into a readable file\n",
    "    f_name = (file_path)\n",
    "    \n",
    "    # open the variable to be read and split into words\n",
    "    with open(f_name, 'r', encoding='utf8') as f:\n",
    "        # read and split into words to count word in file\n",
    "        file_name = f.read()\n",
    "        \n",
    "        # read the files with panda\n",
    "        df_ = pd.read_csv(f_name, sep=\"\\t\", names=[\"TOK\"])\n",
    "        table_ = pd.DataFrame(df_)\n",
    "        \n",
    "        # split the column TOK in two GOLD with the IOB format and TOK_ with the tokenized text\n",
    "        table_[['TOKEN','IOB']] = table_.TOK.str.split(\" \",expand=True,)\n",
    "\n",
    "        # delete the column from TABLE_HAND previews columns\n",
    "        del table_ ['TOK']\n",
    "        del table_ ['IOB']\n",
    "        \n",
    "        # delete the index\n",
    "        blankIndex=[''] * len(table_)\n",
    "        table_.index=blankIndex\n",
    "        \n",
    "        # it allows to display entire table\n",
    "        pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None )\n",
    "        #print(table_)\n",
    "        \n",
    "        # directory out\n",
    "        output_dir = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_10_tokenized/\"\n",
    "        # new files out with original's name plus _text and its new format .txt\n",
    "        results_file = \"%s%s_TOKENIZED.tsv\"%(output_dir, os.path.splitext(os.path.basename(f_hand))[0])\n",
    "        print(results_file)\n",
    "        # write to files\n",
    "        with open(results_file,'w') as write_csv:\n",
    "            write_csv.write(table_hand.to_csv(sep='\\t', index=False))\n",
    "            print(write_csv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0849c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512f19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa08965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9301bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# regular expression\n",
    "import re\n",
    "# retrives file support\n",
    "import glob\n",
    "# make a table\n",
    "import pandas as pd\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "# SpaCy NLP\n",
    "import spacy\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.lang.fr import French\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('fr_dep_news_trf')\n",
    "except OSError:\n",
    "    #print(\"Do you have the spacy module fr_core_news_sm installed? If not run 'python -m spacy download fr_core_news_sm'\")\n",
    "    \n",
    "    def custom_tokenize(text):\n",
    "        words = text.split('|')\n",
    "        return Doc(nlp.vocab, words=words)\n",
    "    nlp.tokenizer = custom_tokenize\n",
    "\n",
    "    # inform the path\n",
    "    for file_path in glob.iglob('tokenized_text/*.txt'): \n",
    "        #transform path into a readable file\n",
    "        f_name = (file_path)\n",
    "        #print(f_name)\n",
    "\n",
    "        # open the variable to be read and split into words\n",
    "        with open(f_name, 'r', encoding='utf8') as f:\n",
    "            # read and split into words to count word in file\n",
    "            #text = f.read()\n",
    "\n",
    "            words = f.read().replace('\\n', '|').replace('||','|').replace('||','|')\n",
    "            words = words[:-1] #remove last pipe\n",
    "            #print(words)\n",
    "\n",
    "            # apply SpaCy\n",
    "            doc = nlp(words)\n",
    "\n",
    "            entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "            tags = offsets_to_biluo_tags(doc, entities)\n",
    "\n",
    "            # directory out\n",
    "            output_dir = \"SpaCy_bert/\"\n",
    "            # new files out with original's name plus _text and its new format .txt\n",
    "            results_file = \"%s%s_SpaCy_bert.tsv\"%(output_dir, os.path.splitext(os.path.basename(f_name))[0])\n",
    "            print(results_file)\n",
    "\n",
    "\n",
    "            # save it as blabla_text.tok\n",
    "            count = 0\n",
    "            with open(results_file, 'w', encoding='utf8') as fpout: \n",
    "                for tok in words.split(\"|\"):\n",
    "                    tag = str(tags[count])\n",
    "                    fpout.write(str(tok)+\"\\t\"+tag+\"\\n\")\n",
    "                    count = count + 1\n",
    "                fpout.close()\n",
    "            #print(\"fini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e163df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d07ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# regular expression\n",
    "import re\n",
    "# retrives file support\n",
    "import glob\n",
    "# make a table\n",
    "import pandas as pd\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "#### HAND TEXT ####\n",
    "\n",
    "# inform the path\n",
    "for file_path in glob.iglob('/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/hand_pandas/*txt'):\n",
    "    #transform path into a readable file\n",
    "    f_hand = (file_path)\n",
    "    print(f_hand)\n",
    "    \n",
    "    # open the variable to be read and split into words\n",
    "    with open(f_hand, 'r', encoding='utf8') as f:\n",
    "        # read and split into words to count word in file\n",
    "        t_hand = f.read()\n",
    "        #print(t_hand)\n",
    "        \n",
    "        #### PANDAS ####\n",
    "        \n",
    "        # read the files with panda\n",
    "        df_hand = pd.read_csv(f_hand, sep=\"\\t\", names=[\"TOK\"])\n",
    "        #print(f_hand)\n",
    "        \n",
    "        table_hand = pd.DataFrame(df_hand)\n",
    "        #print(table_hand)\n",
    "        \n",
    "        # split the column TOK in two GOLD with the IOB format and TOK_ with the tokenized text\n",
    "        table_hand[['TOKEN','IOB']] = table_hand.TOK.str.split(\" \",expand=True,)\n",
    "        #print(table_hand)\n",
    "        \n",
    "        # delete the column from TABLE_HAND previews columns\n",
    "        del table_hand ['TOK']\n",
    "        #print(table_hand)\n",
    "        \n",
    "        # delete the index\n",
    "        blankIndex=[''] * len(table_hand)\n",
    "        table_hand.index=blankIndex\n",
    "        #print(table_hand)\n",
    "        \n",
    "        # it allows to display entire table\n",
    "        pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None )\n",
    "        print(table_hand)\n",
    "        \n",
    "        \"\"\"   \n",
    "        #output\n",
    "        \n",
    "        # directory out\n",
    "        output_dir = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_10_tokenIOB/\"\n",
    "        # new files out with original's name plus _text and its new format .txt\n",
    "        results_file = \"%s%s_IOB_TOKEN.tsv\"%(output_dir, os.path.splitext(os.path.basename(f_hand))[0])\n",
    "        print(results_file)\n",
    "        # write to files\n",
    "        with open(results_file,'w') as write_csv:\n",
    "            write_csv.write(table_hand.to_csv(sep='\\t', index=False))\n",
    "            print(write_csv)\n",
    "            \n",
    "            \"\"\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbff8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "with open (\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_8_SpaCy_results/SpaCy_prediction/prediction_lg/BNU_01_Didier_text_SpaCy_lg_prediction.tsv\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad346fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Measure\tent_type\tpartial\tstrict\texact\"\n",
      "\"correct\t143\t135\t135\t135\"\n",
      "\"incorrect\t0\t0\t8\t8\"\n",
      "\"partial\t0\t8\t0\t0\"\n",
      "\"missed\t60\t60\t60\t60\"\n",
      "\"spurious\t88\t88\t88\t88\"\n",
      "\"possible\t203\t203\t203\t203\"\n",
      "\"actual\t231\t231\t231\t231\"\n",
      "\"precision\t0.619\t0.602\t0.584\t0.584\"\n",
      "\"recall\t0.704\t0.685\t0.665\t0.665\"\n",
      "\"f1\t0.659\t0.641\t0.622\t0.622\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open the file\n",
    "with open (\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_8_SpaCy_results/SpaCy_precision_rappel_LOC_PER/lg_PER_average.tsv\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file\n",
    "with open (\"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/comparaison.txt\", \"r\", encoding=\"utf_8\") as file:\n",
    "    # read the file\n",
    "    text = file.read()\n",
    "    # print it \n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# regular expression\n",
    "import re\n",
    "# retrives file support\n",
    "import glob\n",
    "# make a table\n",
    "import pandas as pd\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "\n",
    "# inform the path\n",
    "for file_path in glob.iglob('/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_8_SpaCy_results/SpaCy_precision_rappel_LOC_PER/*.tsv'):\n",
    "    #transform path into a readable file\n",
    "    f_name = (file_path)\n",
    "    print(f_name)\n",
    "    \n",
    "    # open the variable to be read and split into words\n",
    "    with open(f_name, 'r', encoding='utf8') as f:\n",
    "        # read and split into words to count word in file\n",
    "        t_print = f.read()\n",
    "        print(t_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b598b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize the data with Streamlit and spaCy.\"\"\"\n",
    "import streamlit as st\n",
    "from spacy import displacy\n",
    "import srsly\n",
    "import typer\n",
    "\n",
    "file_paths = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/savoirs_NER/training/model-best/meta.json\"\n",
    "\n",
    "\n",
    "#@st.cache(allow_output_mutation=True)\n",
    "\"\"\"\n",
    "def load_data(filepath):\n",
    "    examples = list(srsly.read_json(filepath))\n",
    "    rows = []\n",
    "    n_total_ents = 0\n",
    "    n_no_ents = 0\n",
    "    labels = set()\n",
    "    for eg in examples:\n",
    "        row = {\"text\": eg[\"text\"], \"ents\": eg.get(\"spans\", [])}\n",
    "        n_total_ents += len(row[\"ents\"])\n",
    "        if not row[\"ents\"]:\n",
    "            n_no_ents += 1\n",
    "        labels.update([span[\"label\"] for span in row[\"ents\"]])\n",
    "        rows.append(row)\n",
    "    return rows, labels, n_total_ents, n_no_ents\n",
    "    \"\"\"\n",
    "\n",
    "def main(file_paths: str):\n",
    "    files = [p.strip() for p in file_paths.split(\",\")]\n",
    "    st.sidebar.title(\"Data visualizer\")\n",
    "    st.sidebar.markdown(\n",
    "        \"Visualize the annotations using [displaCy](https://spacy.io/usage/visualizers) \"\n",
    "        \"and view stats about the datasets.\"\n",
    "    )\n",
    "    data_file = st.sidebar.selectbox(\"Dataset\", files)\n",
    "    data, labels, n_total_ents, n_no_ents = load_data(data_file)\n",
    "    displacy_settings = {\n",
    "        \"style\": \"ent\",\n",
    "        \"manual\": True,\n",
    "        \"options\": {\"colors\": {label: \"#d1bcff\" for label in labels}},\n",
    "    }\n",
    "    st.header(f\"{data_file} ({len(data)})\")\n",
    "    wrapper = \"<div style='border-bottom: 1px solid #ccc; padding: 20px 0'>{}</div>\"\n",
    "    for row in data:\n",
    "        html = displacy.render(row, **displacy_settings).replace(\"\\n\\n\", \"\\n\")\n",
    "        st.markdown(wrapper.format(html), unsafe_allow_html=True)\n",
    "\n",
    "    st.sidebar.markdown(\n",
    "        f\"\"\"\n",
    "    | `{data_file}` | |\n",
    "    | --- | ---: |\n",
    "    | Total examples | {len(data):,} |\n",
    "    | Total entities | {n_total_ents:,} |\n",
    "    | Examples with no entities | {n_no_ents:,} |\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        typer.run(main)\n",
    "    except SystemExit:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183022d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation system\n",
    "import os\n",
    "# retrives file support\n",
    "import glob\n",
    "\n",
    "# input directory\n",
    "input_dir = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/db_10_tokenIOB/\"    \n",
    "\n",
    "### ALL outputs\n",
    "# output directory train\n",
    "output_dir_train = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/13_db_spaCY_trainning/train_data/\"\n",
    "# output directory val\n",
    "output_dir_val = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/13_db_spaCY_trainning/eval_data/\"\n",
    "# output directory test\n",
    "output_dir_test = \"/Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/13_db_spaCY_trainning/test_data/\"\n",
    "\n",
    "# open all tsv files in this directory \n",
    "for file_name in glob.glob(input_dir + \"*.tsv\"):\n",
    "    #print(file_name)\n",
    "    with open(file_name, \"r\", encoding=\"UTF-8\") as fp:\n",
    "        print_f = fp.read()\n",
    "        #print(print_f)\n",
    "        \n",
    "        # counting the number of lines\n",
    "        num_lines = len(print_f.split(\"\\n\"))\n",
    "        \n",
    "        # stablishing the metrics of divison for each file \n",
    "        train_r, val_r, test_r = num_lines*0.7, num_lines*0.2, num_lines*0.1\n",
    "        \n",
    "        # salving the results with their original's name plus its new categories\n",
    "        f_train = open(\"%s%s_train.tsv\"%(output_dir_train, os.path.splitext(os.path.basename(file_name))[0]), \"w\", encoding=\"UTF-8\")\n",
    "        f_val = open(\"%s%s_val.tsv\"%(output_dir_val, os.path.splitext(os.path.basename(file_name))[0]), \"w\", encoding=\"UTF-8\")\n",
    "        f_test = open(\"%s%s_test.tsv\"%(output_dir_test, os.path.splitext(os.path.basename(file_name))[0]), \"w\", encoding=\"UTF-8\")   \n",
    "        \n",
    "        count = 0\n",
    "        # parsing each line\n",
    "        lines2 = print_f.split(\"\\n\")\n",
    "        # loop for division\n",
    "        for l2 in lines2:\n",
    "            #print(count)\n",
    "            if count < train_r:\n",
    "                f_train.write(l2+\"\\n\")\n",
    "                #print(\"in train: \"+l2+\"\\n\")\n",
    "            elif count >= train_r and count < train_r + val_r:\n",
    "                f_val.write(l2+\"\\n\")\n",
    "                #print(\"in val: \"+l2+\"\\n\")\n",
    "            else:\n",
    "                f_test.write(l2+\"\\n\")\n",
    "                #print(\"in test: \"+l2+\"\\n\")\n",
    "            count = count + 1\n",
    "            #print(count)\n",
    "        \n",
    "        # closing the variable result\n",
    "        f_train.close()\n",
    "        f_val.close()\n",
    "        f_test.close()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a3d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ada40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in *; do echo $d ; python -m spacy convert $d /Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/13_db_spaCY_trainning/train_data/ /Users/alexsoares/Desktop/EHESS/dev/Savoirs_env/14_db_json_spaCy/train_json/ -t json -s -c iob ; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy convert /input_file.tsv /output_file -t spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710f576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761abc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy train config.cfg --output ./training \n",
    "                            --paths.train ./corpus/train.spacy \n",
    "                            --paths.dev ./corpus/dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97d2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018181e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54140dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953c662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3fe57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#File: tree-md\n",
    "\n",
    "tree=$(tree -tf --noreport -I '*~' --charset ascii $1 |\n",
    "       sed -e 's/| \\+/  /g' -e 's/[|`]-\\+/ */g' -e 's:\\(* \\)\\(\\(.*/\\)\\([^/]\\+\\)\\):\\1[\\4](\\2):g')\n",
    "\n",
    "printf \"# Project tree\\n\\n${tree}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e731db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734b08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://isolution.pro/fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://isolution.pro/fr/t/pytorch/pytorch-universal-workflow-of-machine-learning/flux-de-travail-universel-de-l-apprentissage-automatique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee620d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy project run all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbea32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://ichi.pro/fr/comment-le-traitement-du-langage-naturel-pnl-changera-t-il-le-monde-143158135786453\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb9cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f788e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://algorithmia.com/blog/introduction-to-loss-functions\n",
    "    https://www.youtube.com/watch?v=IVVVjBSk9N0\n",
    "        https://github.com/llSourcell/loss_functions_explained\n",
    "            https://rohanvarma.me/Loss-Functions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af20d7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9b7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa3d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
