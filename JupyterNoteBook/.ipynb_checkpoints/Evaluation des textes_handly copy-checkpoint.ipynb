{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ffa36a",
   "metadata": {},
   "source": [
    "# to increase the capacite of view in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce6688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increases the jupyter notebook capacity of data\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54f5a0",
   "metadata": {},
   "source": [
    "# 1 - Step\n",
    "## print all files name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a888960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrives file support\n",
    "import glob\n",
    "# inform the path\n",
    "for file_path in glob.iglob('clean_pandas/*.txt'):\n",
    "    file_name = (file_path)\n",
    "    #print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b0933",
   "metadata": {},
   "source": [
    "## 1.a\n",
    "### visualize the fille side by side to compare the errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9579053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table\n",
    "import pandas as pd\n",
    "\n",
    "df_clean = pd.read_csv(\"clean_pandas/LDSII_52_Kassow_OnegShabbes_text_clean_BIO.txt\", \n",
    "                       sep=\"\\t\", names=[\"TOKEN\", \"PREDICTION\", \"GOLD\", \"VALIDITY\"])\n",
    "df_hand = pd.read_csv(\"hand_pandas/LDSII_52_Kassow_OnegShabbes_text_beyond_hand_BIO.txt\", \n",
    "                      sep=\"\\t\", names=[\"TOK\"])\n",
    "\n",
    "# convert the dictionary into DataFrame\n",
    "table_clean = pd.DataFrame(df_clean)\n",
    "table_hand = pd.DataFrame(df_hand)\n",
    "\n",
    "# split the column TOK in two GOLD with the IOB format and TOK_ with the tokenized text\n",
    "table_hand[['TOK_','GOLD_bio']] = table_hand.TOK.str.split(\" \",expand=True,)\n",
    "\n",
    "#df['columnF'] = pd.Series(df1['columnF'])\n",
    "table_clean['GOLD']=pd.Series(table_hand['GOLD_bio'])\n",
    "\n",
    "# delete the index\n",
    "blankIndex=[''] * len(table_clean)\n",
    "table_clean.index=blankIndex\n",
    "\n",
    "# delete the column from TABLE_HAND previews columns\n",
    "del table_hand ['TOK']\n",
    "del table_hand ['TOK_']\n",
    "\n",
    "# it allows to display entire table\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None )\n",
    "\n",
    "# it allows to see the columns name or head \n",
    "#print(df.columns)\n",
    "\n",
    "# It makes able to see the first's four lines\n",
    "#print(table_clean.head())\n",
    "\n",
    "# function to compare columns NE-COARSE-LIT x GOLD\n",
    "def compare(x):    \n",
    "    return '1' if x['PREDICTION'] == x['GOLD'] else 'O'\n",
    "\n",
    "table_clean['VALIDITY'] = table_clean.apply(compare, axis=1)\n",
    "\n",
    "#print(table_hand)\n",
    "#print(table_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753632a",
   "metadata": {},
   "source": [
    "# 2 - Step\n",
    "\n",
    "## Textes problèmatiques \n",
    "### problèmes avec des langues (greek, japonais, latin, chinois)\n",
    "### problèmes des Upper case\n",
    "### problèmes avec les chifres et les tableaux\n",
    "### problèmes avec certains signes (<,>,«,», )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7986667c",
   "metadata": {},
   "source": [
    "BNU_02_Colin_\n",
    "\n",
    "BNU_05_Dege_\n",
    "\n",
    "CMA_11_2018_Saint-Martin_\n",
    "\n",
    "CMA_8_Buccheri_\n",
    "\n",
    "LDSI_04_Dugast_\n",
    "\n",
    "LDSI_08_Glassner_\n",
    "\n",
    "LDSII_04_Icher_LesOutilsDuCompagnon_\n",
    "\n",
    "LDSII_09_Icher_LetabliDuMenuisier_\n",
    "\n",
    "LDSII_19_Ragazzoli_LireInscrireSurvivreEgypteAncienne_\n",
    "\n",
    "LDSII_27_Dejean_LaStructurationDocumentsElectroniques_\n",
    "\n",
    "LDSII_33_Berra_Thesaurus_\n",
    "\n",
    "LDSII_38_Houdart_ArchitectureTrompeLoeil_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10bdf9c",
   "metadata": {},
   "source": [
    "# 3 - Step\n",
    "## read the csv file ( precision_rappel )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d763a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure\tent_type\tpartial\tstrict\texact\n",
      "correct\t0\t0\t0\t0\n",
      "incorrect\t1\t0\t1\t1\n",
      "partial\t0\t1\t0\t0\n",
      "missed\t35\t35\t35\t35\n",
      "spurious\t86\t86\t86\t86\n",
      "possible\t36\t36\t36\t36\n",
      "actual\t87\t87\t87\t87\n",
      "precision\t0.0\t0.006\t0.0\t0.0\n",
      "recall\t0.0\t0.014\t0.0\t0.0\n",
      "f1\t0\t0.008\t0\t0\n"
     ]
    }
   ],
   "source": [
    "# libray to read csv\n",
    "import csv\n",
    "\n",
    "with open('db_results/SpaCy_precision_rappel/precision_rappel_BERT/BNU_01_Didier_text_SpaCy_bert_BIO_prediction_precision_rappel.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91390769",
   "metadata": {},
   "source": [
    "## 3.a \n",
    "### read all file from the database\n",
    "### concatenate them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f439b1fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d9fa4188ad62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_from_each_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_from_each_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#df_merged.to_csv( \"db_results/SpaCy_precision_rappel/all_merged_BERT.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EHESS/dev/env/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EHESS/dev/env/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = r'db_results/SpaCy_precision_rappel/precision_rappel_BERT/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in all_files)\n",
    "df_merged = pd.concat(df_from_each_file, ignore_index=True)\n",
    "#df_merged.to_csv( \"db_results/SpaCy_precision_rappel/all_merged_BERT.csv\")\n",
    "\n",
    "#print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e825974",
   "metadata": {},
   "source": [
    "## 3.b\n",
    "### create the avarage all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c8abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Measure\\tent_type\\tpartial\\tstrict\\texact\n",
      "0               correct\\t204\\t222\\t190\\t222\n",
      "1                  incorrect\\t35\\t0\\t49\\t17\n",
      "2                      partial\\t0\\t17\\t0\\t0\n",
      "3                    missed\\t36\\t36\\t36\\t36\n",
      "4              spurious\\t167\\t167\\t167\\t167\n",
      "5              possible\\t275\\t275\\t275\\t275\n",
      "6                actual\\t406\\t406\\t406\\t406\n",
      "7     precision\\t0.502\\t0.568\\t0.468\\t0.547\n",
      "8        recall\\t0.742\\t0.838\\t0.691\\t0.807\n",
      "9            f1\\t0.599\\t0.677\\t0.558\\t0.652\n"
     ]
    }
   ],
   "source": [
    "average = df.groupby(df.columns, axis=1).sum()\n",
    "print(average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf069772",
   "metadata": {},
   "source": [
    "## 3.c\n",
    "### save it as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164c4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "# write to file\n",
    "with open(\"db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv\",'w') as write_csv:\n",
    "    \n",
    "    write_csv.write(average.to_csv(sep='\\t', index=False))\n",
    "    print(write_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c49c2c",
   "metadata": {},
   "source": [
    "## 3.d\n",
    "### print it to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5590f33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa9f62a430a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mspamreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspamreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv'"
     ]
    }
   ],
   "source": [
    "# libray to read csv\n",
    "import csv\n",
    "\n",
    "with open('db_results/SpaCy_precision_rappel/BERT_all_text_average.tsv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        (', '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62078d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
